---
title: 如何在本機上跑Vicuna模型
tags: Tool Natural-Language-Processing
layout: article
footer: false
aside:
  toc: true
mathjax_autoNumber: true
published: true
---

隨著LLaMA的推出，有越來越多人基於該模型做了各式各樣的調校，而Vicuna是其中的一種，這篇文章記錄一下如何在自己的機器上面將Vicuna跑起來。

<!--more-->

## Vicuna

[Vicuna](https://vicuna.lmsys.org/)是一個基於[LLaMA](https://github.com/facebookresearch/llama)的大型語言模型（LLM），開發團隊使用GPT-4來當作評審去評論Vicuna和ChatGPT的回答哪個比較好，而Vicuna在測試裡面達到了90%的水準，是個表現相當不錯的模型，個人覺得回答的效果比起[Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html)和單純的LLaMA還要好上不少。

底下會講一下如何在本機上面執行Vicuna，在ChatGPT跑不出你想要的結果時，不妨嘗試將問題問看看Vicuna。

## 下載LLaMA

由於Vicuna是基於LLaMA的模型，首先我們會需要先拿到LLaMA的參數，正規的方式是去這份[google表單](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform)上面填寫資料，跟Meta申請使用許可，不過網路社群上也提供了很多其他的方式來讓大家下載，像是[pyllama](https://github.com/juncongmoo/pyllama)。

```bash
pip3 install pyllama -U
python -m llama.download --model_size 13B --folder /tmp/pyllama_data
```

只要下上面的指令就能簡單下載到LLaMA 13B的參數了，詳細的說明可以參考pyllama的文件。

在執行的過程中有可能連線會卡住，可以使用`Ctrl+C`將連線斷開以後執行第二行的指令，它會從斷掉的地方繼續下載，不用怕會需要從頭下載，下載下來的參數僅限於研究的用途，不應該再被拿去做其他使用。

## 轉成huggingface的格式

在下載好LLaMA的參數以後，我們需要將下載下來的參數轉成hugging face的格式，為此我們需要安裝`transformers`在機器上，而Vicuna要求transformers的版本要在**4.28.0**以上，如果直接用pip安裝的版本不夠高的話，可以直接下下面的指令來安裝最新版。

```bash
pip3 install https://github.com/huggingface/transformers/archive/refs/heads/main.zip
```

接著就能將LLaMA轉成hugging face的格式了

```bash
python3 ./convert_llama_weights_to_hf.py \
    --input_dir ./pyllama_data \
    --model_size 13B \
    --output_dir ./llama_hf
```

上面指令裡面的`convert_llama_weights_to_hf.py`是使用[LLaMA在hugging face頁面上](https://huggingface.co/docs/transformers/main/model_doc/llama)所提供的[這份](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py)，而`./pyllama_data`是上面使用pyllama下載下來的資料夾、`./llama_hf`是輸出的資料夾。

## 下載Vicuna delta

Vicuna的參數放在他們的[hugging face帳戶上](https://huggingface.co/lmsys)，但因為裡面有包含了非常大檔案的參數檔，所以在使用git clone下來之前會需要先安裝[Git Large File Storage（LFS）](https://git-lfs.com/)才能將大檔案下載下來，底下是在Cent OS上安裝並下載的指令。

```bash
curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.rpm.sh | sudo bash
sudo yum install -y git-lfs
git lfs install
git clone https://huggingface.co/lmsys/vicuna-13b-delta-v1.1
```

## Apply Vicuna delta

有了LLaMA的參數和Vicuna的delta後，我們需要將delta套用在LLaMA的參數上來獲得實際的Vicuna，套用的方法是使用pip `fschat`的套件。

```bash
pip3 install fschat
python3 -m fastchat.model.apply_delta \
    --base ./llama_hf \
    --target ./vicuna-13b \
    --delta ./vicuna-13b-delta-v1.1
```

使用套件裡面的`apply_delta`搭配先前準備好的hugging face格式的LLaMA和Vicuna delta就能獲得實際的Vicuna了，值得一提的是套用7B的模型需要30GB的RAM，而13B的模型需要60GB，如果RAM沒有那麼足夠的話，可以參考[文件當中的方式](https://github.com/lm-sys/FastChat#low-cpu-memory-conversion)來降低記憶體的使用。

## 使用Vicuna

準備好Vicuna得參數後，便能使用fastchat提供的function來在terminal上面跟Vicuna對話了，詳細的參數可以參考[文件](https://github.com/lm-sys/FastChat#inference-with-command-line-interface)。

```bash
python3 -m fastchat.serve.cli --model-path ./vicuna-13b-delta-v1.1
```

如果想用網頁版的方式來互動，fastchat也有使用[Gradio](https://gradio.app/)來包成網頁，詳細的說明請參考[文件](https://github.com/lm-sys/FastChat#serving-with-web-gui)。

Vicuna有另外推出了[FastChat-T5](https://github.com/lm-sys/FastChat#fastchat-t5)供大家在商業上使用，指令跟上面文章的相同，只需要將模型的路徑指到FastChat-T5就行了。
