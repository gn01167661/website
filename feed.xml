<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.6">Jekyll</generator><link href="https://wjohn1483.github.io/#%20the%20base%20hostname%20&%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://wjohn1483.github.io/#%20the%20base%20hostname%20&%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/" rel="alternate" type="text/html" /><updated>2020-04-19T03:34:38+00:00</updated><id>https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/</id><title type="html">wjohn1483.github.io</title><subtitle></subtitle><author><name>Your Name</name></author><entry><title type="html">Generative Adversarial User Model for Reinforcement Learning Based Recommendation System</title><link href="https://wjohn1483.github.io/#%20the%20base%20hostname%20&%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2020/04/19/generative-adversarial-user-model-for-reinforcement-learning-based-recommendation-system/" rel="alternate" type="text/html" title="Generative Adversarial User Model for Reinforcement Learning Based Recommendation System" /><published>2020-04-19T00:00:00+00:00</published><updated>2020-04-19T00:00:00+00:00</updated><id>https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2020/04/19/generative-adversarial-user-model-for-reinforcement-learning-based-recommendation-system</id><content type="html" xml:base="https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2020/04/19/generative-adversarial-user-model-for-reinforcement-learning-based-recommendation-system/">&lt;p&gt;Reinforcement Learning很適合用在推薦系統上，然而要訓練RL需要很多跟使用者互動的經驗，這篇paper使用類似GAN的方法來去創造user model，以跟推薦系統互動。&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;簡介&quot;&gt;簡介&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1812.10613.pdf&quot;&gt;這篇paper&lt;/a&gt;是ICML 2019的oral，主要是在講述，如何在推薦系統上套用Reinforcement Learning的框架，為此需要建立一個user model當作是環境，好讓推薦系統與之互動，作者還提出了Cascading Q-Network來做組合的推薦。&lt;/p&gt;

&lt;p&gt;如果想對這篇paper瞭解更多的話，可以參考&lt;a href=&quot;https://icml.cc/media/Slides/icml/2019/201(11-14-00)-11-14-25-4831-generative_adve.pdf&quot;&gt;作者在Oral的投影片&lt;/a&gt;和&lt;a href=&quot;https://slideslive.com/38917397/time-series&quot;&gt;演講錄影&lt;/a&gt;(從25:50開始)。&lt;/p&gt;

&lt;h2 id=&quot;方法&quot;&gt;方法&lt;/h2&gt;

&lt;h3 id=&quot;推薦系統與環境的互動方式&quot;&gt;推薦系統與環境的互動方式&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;interaction.png&quot; alt=&quot;Interaction&quot; /&gt;&lt;/p&gt;

&lt;p&gt;一開始作者先簡單介紹了一下推薦系統跟環境(user)互動的方式&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;我們可以先從過去的資料蒐集該使用者過去點選過的東西，也就是上圖左下角的&lt;code class=&quot;highlighter-rouge&quot;&gt;state at t&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;推薦系統會根據state推薦出一些使用者可能有興趣的東西&lt;/li&gt;
  &lt;li&gt;使用者從中選擇出他有興趣的物品，或是選擇以上皆沒有興趣&lt;/li&gt;
  &lt;li&gt;將使用者新的選擇放入歷史資料中&lt;/li&gt;
  &lt;li&gt;算出新的state以後回到步驟2&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;兩者互動的方式便是推薦系統不斷推薦物品，而使用者不斷從中選擇他有興趣的東西，如此反覆來創造訓練資料。&lt;/p&gt;

&lt;h3 id=&quot;generative-adversarial-user-model&quot;&gt;Generative Adversarial User Model&lt;/h3&gt;

&lt;p&gt;在前面有提到，訓練RL需要與環境互動很多次才能訓練得好，然而這在推薦系統上是一件難以做到的事情，因為推薦系統的環境便是真實的使用者，而真實的使用者並沒有那麼的有耐心，願意陪你訓練推薦系統。另一個用RL訓練推薦系統的難處是，我們沒辦法去量化使用者的reward，我們只能從使用者的行為判斷出他可能比較喜歡哪一個物品，卻沒有辦法知道他有多喜歡，舉例來說，我們推出了十個物品給使用者，使用者點選了其中一個，接著又推出另外十個物品給使用者，使用者又點選了其中一個，雖然使用者在兩次的推薦當中都有點選物品，但我們無法知道使用者是不是喜歡第一次推薦的物品大過第二次推薦的物品。&lt;/p&gt;

&lt;p&gt;為此，作者訓練了一個使用者模型來去模擬真實使用者的選擇，並產生出量化的reward，方便推薦系統做訓練。&lt;/p&gt;

&lt;h4 id=&quot;framework&quot;&gt;Framework&lt;/h4&gt;

&lt;p&gt;&lt;img class=&quot;image image--xl&quot; src=&quot;gan-user-model.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;身為一個使用者，他會看到推薦系統所推薦的物品清單&lt;script type=&quot;math/tex&quot;&gt;A^t&lt;/script&gt;，generator會根據過去的選擇(使用者的喜好)，選擇出他有興趣的物品&lt;script type=&quot;math/tex&quot;&gt;a^t\sim \phi(s^t, A^t)&lt;/script&gt;，而discriminator想要做的便是對generator給出的物品打個reward分數&lt;script type=&quot;math/tex&quot;&gt;r(s^t,a^t)&lt;/script&gt;。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\phi(s^t, A^t)=\arg\max\limits_{\phi}\mathbb{E}_\phi[r(s^t,a^t)]-R(\phi)/\eta&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;r_\theta(s^t,a^t)=\mathbf{v}\sigma(\mathbf{V}(s^t,f_{a^t})+b)&lt;/script&gt;

&lt;p&gt;式(1)中的&lt;script type=&quot;math/tex&quot;&gt;R(\phi)/\eta&lt;/script&gt;是一個regularisation term，generator希望能獲取最大的reward，而reward其實就是將使用者過去的選擇和商品的feature餵入一層NN而得到一個純量，兩者想要最佳化的式子如下&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min\limits_{\theta}\max\limits_{\phi}\left( \mathbb{E}_\phi[\sum^T_{t=1}r_\theta(s^t_{true}, a^t)]-R(\theta)/\eta\right) - \sum^T_{t=1}r_\theta(s^t_{true},a^t_{true})&lt;/script&gt;

&lt;p&gt;擁有下標&lt;script type=&quot;math/tex&quot;&gt;true&lt;/script&gt;的代表是使用者真實的選擇，而generator想要最佳化的是第一項，discriminator想要最佳化的是第二項，也就是說generator想要讓自己產生出的選擇跟使用者真正的選擇越貼近越好，而discriminator想要讓使用者真正的選擇所得到的reward越大越好。&lt;/p&gt;

&lt;h4 id=&quot;user-state&quot;&gt;User State&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;user-model-state.png&quot; alt=&quot;State of user model&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在generator和discriminator都有用同使用到的&lt;script type=&quot;math/tex&quot;&gt;s^t&lt;/script&gt;代表的是使用者過去的選擇，也就是使用者的喜好，其被實作的方式其實就是把過去所選擇的那些物品的feature(文字敘述、圖片等等做成embedding)，通過LSTM或者是直接乘上一個大矩陣並concatenate在一起，就完成了。&lt;/p&gt;

&lt;h3 id=&quot;cascading-q-networks&quot;&gt;Cascading Q-Networks&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;set-recommendation.png&quot; alt=&quot;Set Recommendation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在前面使用類似GAN的方法得到了user model以後，接下來就是做一個推薦系統來從廣大的物品池裡面找出使用者會喜歡的商品，倘若想要使用&lt;script type=&quot;math/tex&quot;&gt;Q-function&lt;/script&gt;來去對每一個可能的組合去做評估的話(式(4))，可以想像得到我們應該是算不完的，因為組合數實在是太多了，假如&lt;script type=&quot;math/tex&quot;&gt;K=3000&lt;/script&gt;、&lt;script type=&quot;math/tex&quot;&gt;k=10&lt;/script&gt;，我們就得要計算&lt;script type=&quot;math/tex&quot;&gt;3000\choose10&lt;/script&gt;這麼多個可能，是故作者推出了Cascading Q-Networks來解決這個問題。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a^*_1, a^*_2, ..., a^*_k=\arg\max Q(s^t, a_1, a_2, ..., a_k)&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;cdqn.png&quot; alt=&quot;Cascading Q-Networks&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其想法很簡單，如果組合數太多的話，那我就一個一個求出該被放進去的物品，直至&lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;個物品都被選擇好。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;cdqn-formula.png&quot; alt=&quot;CDQN formula&quot; /&gt;&lt;/p&gt;

&lt;p&gt;而這個Cascading Q-Networks的訓練方式可以想成是訓練&lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;個DQN，而每個DQN的reward都是一樣的，是user model給予整個組合的分數。&lt;/p&gt;

&lt;h2 id=&quot;實驗&quot;&gt;實驗&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;user-model-number.png&quot; alt=&quot;User Model&quot; /&gt;&lt;/p&gt;

&lt;p&gt;首先作者想要知道用GAN所訓練出來的user model的表現如何，便將這個user model去跟其他推薦系統相比較，可以看到用GAN訓練出來的user model表現蠻好的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;recommendation-performance.png&quot; alt=&quot;Recommendation Performance&quot; /&gt;&lt;/p&gt;

&lt;p&gt;再來是對Cascading Q-Networks的實驗，想要知道以DQN作為推薦系統的表現怎麼樣，也可以看到CDQN的表現也相當亮眼。&lt;/p&gt;

&lt;h2 id=&quot;結論&quot;&gt;結論&lt;/h2&gt;

&lt;p&gt;在這篇paper裡面作者使用GAN的技術來模擬真實的使用者，讓用RL來訓練推薦系統變得沒有那麼遙不可及。&lt;/p&gt;</content><author><name>Your Name</name></author><category term="Paper" /><category term="Generative-Adversarial-Network" /><category term="Recommendation-System" /><summary type="html">Reinforcement Learning很適合用在推薦系統上，然而要訓練RL需要很多跟使用者互動的經驗，這篇paper使用類似GAN的方法來去創造user model，以跟推薦系統互動。</summary></entry><entry><title type="html">有錢人想的和你不一樣</title><link href="https://wjohn1483.github.io/#%20the%20base%20hostname%20&%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2020/04/04/secrets-of-the-millionaire-mind/" rel="alternate" type="text/html" title="有錢人想的和你不一樣" /><published>2020-04-04T00:00:00+00:00</published><updated>2020-04-04T00:00:00+00:00</updated><id>https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2020/04/04/secrets-of-the-millionaire-mind</id><content type="html" xml:base="https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2020/04/04/secrets-of-the-millionaire-mind/">&lt;p&gt;節錄部分&lt;a href=&quot;https://tw.buy.yahoo.com/gdsale/gdsale.asp?gdid=8104317&quot;&gt;《有錢人想的和你不一樣》&lt;/a&gt;這本書的內容以及自己的一些心得。&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;簡介&quot;&gt;簡介&lt;/h2&gt;

&lt;p&gt;一開始是自己想要學著投資，進而在網路上看到了&lt;a href=&quot;https://www.youtube.com/channel/UC45i13dEfEVac2IEJT_Nr5Q&quot;&gt;柴鼠兄弟&lt;/a&gt;和&lt;a href=&quot;https://rich01.com/blog-page_30/&quot;&gt;市場先生的投資書單&lt;/a&gt;的推薦而決定看這本書，覺得這本書講的概念挺好的，是故在此記錄一下，以提醒我自己。&lt;/p&gt;

&lt;h2 id=&quot;第一篇金錢藍圖&quot;&gt;第一篇：金錢藍圖&lt;/h2&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;設定\rightarrow想法\rightarrow感覺\rightarrow行動=結果&lt;/script&gt;

&lt;p&gt;第一篇主要是在講述金錢觀的重要性，每個人年幼或是生長環境的不同（設定），造就了想法不一樣，而想法造成感覺，最終變成行動，也就是現在所處在的結果。聽起來有點抽象但又有點道理，如果我們對現在這個處境不甚滿意，覺得金錢太少、生活不自在或是有各種各樣不舒適的地方，其成因主要是自己過去的行動所造成的，而行動又源自於感覺、想法，書中把想法定義為過去經驗的累積形塑而成。&lt;/p&gt;

&lt;p&gt;也因此在第一篇的內容當中，作者主要在講述要如何去察覺、修改自己過去經驗所帶給你的意義，過去發生的所有事情都只是一個客觀事實，而真的會影響你的是你自己賦予那些客觀事實的&lt;strong&gt;意義&lt;/strong&gt;，這些意義會造就你對事情的看法，進而使你做出不一樣的行動，所以賦予過去經驗正面的意義是相當重要的。&lt;/p&gt;

&lt;h2 id=&quot;第二篇財富檔案&quot;&gt;第二篇：財富檔案&lt;/h2&gt;

&lt;p&gt;第二篇在講述一些好的、正面的想法有哪些，希望讀者們在日常的生活中也能夠實踐這些想法，讓自己有負面想法時可以調整。&lt;/p&gt;

&lt;h3 id=&quot;檔案1我創造我的人生&quot;&gt;檔案1：我創造我的人生&lt;/h3&gt;

&lt;p&gt;這個檔案在講述你現在的境遇是自己所創造的，所以不要去責怪、抱怨，應該要試著去釐清是什麼樣的原因導致事情的發生，不要把自己當成是受害者，會把自己當成是受害者的其中一個原因可能是想要博取他人的注意，但一直想要博得人們的關注和注意是不能獲得快樂的，只是活在別人的恩惠底下。&lt;/p&gt;

&lt;h3 id=&quot;檔案2玩金錢遊戲就是為了贏&quot;&gt;檔案2：玩金錢遊戲就是為了贏&lt;/h3&gt;

&lt;p&gt;這個檔案主要是希望能設定一個遠大的目標，如果只是想要過得舒服，那最終的結果可能也就僅止於舒服，但若目標是想要賺大錢，那最終的結果可能就是超級舒服。&lt;/p&gt;

&lt;p&gt;平常去餐廳吃飯的時候，我總是會以價格來當作參考指標，如果價格超出我的預算或是覺得不想花太多錢，我總是會選擇價格最低的餐點，從來沒有嘗試過被標示為”市價”的餐點，但或許可以以此作為目標，想吃吃看就吃，做到不以價格為考量的境地。&lt;/p&gt;

&lt;h3 id=&quot;檔案3努力讓自己有錢&quot;&gt;檔案3：努力讓自己有錢&lt;/h3&gt;

&lt;p&gt;這邊是在問讀者願意花多少心力來讓自己有錢，讓我想起&lt;a href=&quot;https://www.youtube.com/watch?v=mETU97jbrDE&quot;&gt;很久之前看過的影片&lt;/a&gt;，如果只是半調子的決心，只是想要變得成功、變得有錢，這樣的覺悟是不夠的，你需要做好犧牲時間、睡眠的心理準備，才能讓自己達成目標。&lt;/p&gt;

&lt;h3 id=&quot;檔案4想得很大&quot;&gt;檔案4：想得很大&lt;/h3&gt;

&lt;p&gt;你的收入與市場所認為你產生的價值成正比，假如果你的產出可以影響到數十萬人、數百萬人，那麼你的收入應該相當的驚人，反之，如果只能影響到數十人、數百人，那就應該要試著去提高你所能影響到的人數。這邊的”影響”，不一定是你工作、專業上能夠影響的範圍，也可以是你去投資所造成的影響。&lt;/p&gt;

&lt;h3 id=&quot;檔案5專注於機會&quot;&gt;檔案5：專注於機會&lt;/h3&gt;

&lt;p&gt;很多在面對挑戰的時候，常常會覺得自己還沒有準備好，等自己準備好了再來挑戰，但你永遠不會知道你到底準備好了沒、什麼樣子叫做準備好了呢？&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;「預備、射擊、瞄準！」&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;書中的意思並不是說在毫無準備、研究的情況下就貿然去挑戰，而是給自己一個期限，在期限內盡可能的去準備，而期限一到就試著去挑戰吧！一邊挑戰的同時一邊修正自己的錯誤、方向。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;「行動永遠比不行動來得好。」&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;上面這句是作者的座右銘，並希望能夠在行動的同時練習樂觀並感激你所擁有的事物，若不懂得感激你所擁有的，你將不會得到更多。&lt;/p&gt;

&lt;h3 id=&quot;檔案6欣賞其他有錢人&quot;&gt;檔案6：欣賞其他有錢人&lt;/h3&gt;

&lt;p&gt;我們時常會羨慕、嫉妒別人擁有但自己所沒有的事物，既然別人努力得到了你所沒有的事物，我們應該要祝福而不該酸言酸語，或是討厭他們。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;「祝福你所想要的事物。」&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;檔案7與積極的成功人士交往&quot;&gt;檔案7：與積極的成功人士交往&lt;/h3&gt;

&lt;p&gt;我個人覺得人是很容易被環境所影響的，所以如果有機會的話，應該試著多去跟那些成功人士打交道，並相信自己跟他們是沒有什麼太大區別的，他們做得到的事情，你也做得到。&lt;/p&gt;

&lt;h3 id=&quot;檔案8樂意宣傳自己和自己的價值觀&quot;&gt;檔案8：樂意宣傳自己和自己的價值觀&lt;/h3&gt;

&lt;p&gt;很多人常覺得說，如果我今天做出了一個超級厲害的東西，大家就會搶著來用，但前提是他們得要先知道你做出來了，所以如果有機會的話可以試著多學習宣傳自己，相信自己是個具有高價值，值得被宣傳。&lt;/p&gt;

&lt;h3 id=&quot;檔案9大於問題&quot;&gt;檔案9：大於問題&lt;/h3&gt;

&lt;p&gt;小時候可能覺得騎自行車是一件很困難的事情，但長大了以後就覺得其實這並沒有那麼困難，這是因為你已經成長到了大過於你的問題的程度，所以當你今天遇到了一件你覺得相當困難的事情時，就要提醒自己「我很小！」，該是成長的時候了。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;「成功的秘訣，就是不要逃避問題，不要在問題面前退縮；成功的秘訣就在於你要成長，讓自己大於一切的問題。」&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;檔案10很棒的接受者&quot;&gt;檔案10：很棒的接受者&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;「如果你說你有價值，你就有價值。如果你說你沒有價值，那麼你就沒有。然後你就會依照你的故事版本而活。」&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;我常常會因為自己所做錯的一些事情而認為自己沒有什麼價值，甚至是當別人誇讚的時候也總是著眼於自己沒有做好的部分，然而這樣會使得你越來越悲觀，應該要試著去編一個可以激勵你自己的故事，不要以「下次要小心」作為目標，而是「我想要做到」為目標。&lt;/p&gt;

&lt;h3 id=&quot;檔案11根據結果拿酬勞&quot;&gt;檔案11：根據結果拿酬勞&lt;/h3&gt;

&lt;p&gt;這邊作者希望讀者可以用結果來換取報酬，舉凡常見的工作都是用錢來買你的時間，然而人的時間是有限的，所以能賺取的酬勞也是有限的，是故要用結果來換取酬勞才能得到較多的報酬。&lt;/p&gt;

&lt;h3 id=&quot;檔案12如何兩個都要&quot;&gt;檔案12：如何兩個都要&lt;/h3&gt;

&lt;p&gt;在生活中時常會面臨抉擇，有時選了一個就不能選另一個，但是這有可能是你自我設限，應保持著想辦法兩者兼拿的想法，試著去把所有的好處都拿到。&lt;/p&gt;

&lt;h3 id=&quot;檔案13專注於自己的淨值&quot;&gt;檔案13：專注於自己的淨值&lt;/h3&gt;

&lt;p&gt;我們在談論有錢人排行榜的時候，並不會去看他的收入，而是看他的淨值，所以我們應該著重的是去提升我們的淨值，不單單只看工作收入而已。我們的淨值主要來自於&lt;strong&gt;收入、儲蓄、投資獲利、節流&lt;/strong&gt;，若能在每個部分都有提昇一些，相信淨值會提升不少。&lt;/p&gt;

&lt;h3 id=&quot;檔案14管理自己的錢財&quot;&gt;檔案14：管理自己的錢財&lt;/h3&gt;

&lt;p&gt;有些人會覺得說等我有了一筆錢再來學習怎麼管理，然而管理的習慣應該從現在就開始，從小錢開始培養，否則即使有了一筆錢也會因為不會管理而失去。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;「除非你能管理你現有的一切，否則你不會得到更多！」&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;「管理金錢的習慣比數目更重要。」&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;檔案15讓錢辛苦工作&quot;&gt;檔案15：讓錢辛苦工作&lt;/h3&gt;

&lt;p&gt;在前面有提到，人的時間是有限的，也需要睡覺、休息和玩樂的時間，是故創造被動收入是很重要的，而被動收入有兩個面向&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;讓錢為你工作&lt;/p&gt;

    &lt;p&gt;舉凡像是投資股票、債券、基金等，用錢滾錢的方法&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;讓事業為你工作&lt;/p&gt;

    &lt;p&gt;讓事業自動、系統化，像是書籍的出版稅、音樂版權等，利用你已經創建好的價值，不斷從中收取利益&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;檔案16恐懼也會採取行動&quot;&gt;檔案16：恐懼也會採取行動&lt;/h3&gt;

&lt;p&gt;這裡我覺得是在呼應&lt;a href=&quot;#檔案9大於問題&quot;&gt;檔案9&lt;/a&gt;，人總是會對自己不熟悉的東西產生恐懼，我們要做的事情不是去抹煞掉恐懼，而是接受恐懼並採取行動。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;「如果你只願意做輕鬆的事，人生就會困難重重。但如果你願意做困難的事，人生就會變得很輕鬆。」&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;每當有負面、恐懼的想法出現時，想想前面的學習，意義是自己去賦予的，不如編一個可以幫助自己前進的故事吧。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;「你只在一個狀況下是真正在成長的，那就是你覺得不舒服的時候。」&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;檔案17持續學習成長&quot;&gt;檔案17：持續學習成長&lt;/h3&gt;

&lt;p&gt;「我知道」，是最危險的三個字，我們應該努力學習成長，不應該對自己現有的知識感到自滿，也很有很大的機會我們所以為的跟真實是很不一樣的，如果可以，每天讀一點書，學習一點新知。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;「每一位大師都曾經是個大失敗。」&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name>Your Name</name></author><category term="Book" /><summary type="html">節錄部分《有錢人想的和你不一樣》這本書的內容以及自己的一些心得。</summary></entry><entry><title type="html">TF-IDF簡介</title><link href="https://wjohn1483.github.io/#%20the%20base%20hostname%20&%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2020/04/02/tf-idf-introduction/" rel="alternate" type="text/html" title="TF-IDF簡介" /><published>2020-04-02T00:00:00+00:00</published><updated>2020-04-02T00:00:00+00:00</updated><id>https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2020/04/02/tf-idf-introduction</id><content type="html" xml:base="https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2020/04/02/tf-idf-introduction/">&lt;p&gt;這篇介紹一下常被使用的TF-IDF是什麼，以及怎麼用來做一個簡單的分類器。&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;簡介&quot;&gt;簡介&lt;/h2&gt;

&lt;p&gt;TF-IDF的全稱是&lt;strong&gt;Term Frequency - Inverse Document Frequency&lt;/strong&gt;，可以看&lt;a href=&quot;https://zh.wikipedia.org/wiki/Tf-idf&quot;&gt;它的wiki&lt;/a&gt;有詳細的介紹，簡單來說就是一種統計的方法，可以應用在搜尋引擎上。&lt;/p&gt;

&lt;p&gt;假設今天在搜尋欄上打上了一些字詞，我們想要去尋找網路上有哪些文章跟搜尋欄中的字詞的相關程度最高，而TF-IDF的主要概念就是去看每一篇文章當中該字詞出現的頻率，如果出現的頻率很高的話，就可以說該文章跟搜尋欄的字詞是有高度相關的。&lt;/p&gt;

&lt;h2 id=&quot;方法&quot;&gt;方法&lt;/h2&gt;

&lt;h3 id=&quot;term-frequency-tf&quot;&gt;Term Frequency (TF)&lt;/h3&gt;

&lt;p&gt;假設網路上的文章&lt;script type=&quot;math/tex&quot;&gt;d_j&lt;/script&gt;使用到的字詞種類總共有&lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;個，分別為&lt;script type=&quot;math/tex&quot;&gt;t_{1,d_j}, t_{2,d_j}, ..., t_{k,d_j}&lt;/script&gt;，其中&lt;script type=&quot;math/tex&quot;&gt;t_{1,d_j}&lt;/script&gt;出現了3次，&lt;script type=&quot;math/tex&quot;&gt;t_{2,d_j}&lt;/script&gt;出現了5次等等，我們就可以定義說每個字詞種類&lt;script type=&quot;math/tex&quot;&gt;t_{i,d_j}&lt;/script&gt;對於這篇文章的重要程度為&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;tf_{t_{i,d_j}}=\frac{n_{t_{i,d_j}}}{\sum_k n_{t_{k,d_j}}}&lt;/script&gt;

&lt;p&gt;上面的&lt;script type=&quot;math/tex&quot;&gt;n_{t_{i,d_j}}&lt;/script&gt;指的是字詞種類&lt;script type=&quot;math/tex&quot;&gt;t_{i,d_j}&lt;/script&gt;在文章&lt;script type=&quot;math/tex&quot;&gt;d_j&lt;/script&gt;出現的次數，像&lt;script type=&quot;math/tex&quot;&gt;t_{1,d_j}&lt;/script&gt;在文章中出現了3次，那&lt;script type=&quot;math/tex&quot;&gt;n_{t_{i,d_j}}&lt;/script&gt;就等於3，這個式子的概念其實就是計算這個字詞種類佔了這篇文章多少比重，假設文章&lt;script type=&quot;math/tex&quot;&gt;d_j&lt;/script&gt;總共有100個字(亦即&lt;script type=&quot;math/tex&quot;&gt;\sum_k n_{t_{k,d_j}}=100&lt;/script&gt;)，其中&lt;script type=&quot;math/tex&quot;&gt;t_{1,d_j}&lt;/script&gt;出現了3次，那&lt;script type=&quot;math/tex&quot;&gt;t_{1,d_j}&lt;/script&gt;在這篇文章的重要程度就是&lt;script type=&quot;math/tex&quot;&gt;3/100&lt;/script&gt;。&lt;/p&gt;

&lt;p&gt;由上面的式子算出了每個字詞種類&lt;script type=&quot;math/tex&quot;&gt;t_{i,d_j}&lt;/script&gt;對文章&lt;script type=&quot;math/tex&quot;&gt;d_j&lt;/script&gt;的重要程度以後，我們可以接著算出各個文章&lt;script type=&quot;math/tex&quot;&gt;d_1, d_2, ..., d_l&lt;/script&gt;中各個字詞種類對該文章的重要程度，得到一堆的term frequency。&lt;/p&gt;

&lt;h3 id=&quot;inverse-document-frequency-idf&quot;&gt;Inverse Document Frequency (IDF)&lt;/h3&gt;

&lt;p&gt;如果只考慮字詞出現的次數的話，有可能會被比較一般常用的字詞，像是助詞”的”、”了”所影響，可以想像我在搜尋欄打上”國王的新衣”，因為”的”在每個文章出現的次數都很多，所以只單看term frequency的話，找出來的文章應該都不會是我所想要的。&lt;/p&gt;

&lt;p&gt;為此需要再多考慮一個inverse document frequency，也就是某個字詞在文章間的出現頻率，假如”國王”在全部1000個文章裡面只出現在10篇文章裡，那這10篇文章想必是比其他文章還要來得接近我想要找的東西，inverse document frequency定義如下&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;idf_{t_i}=\frac{\vert D\vert}{\vert \{ j:t_i\in d_j \}\vert}&lt;/script&gt;

&lt;p&gt;上面式子的意思是，某個字詞&lt;script type=&quot;math/tex&quot;&gt;t_i&lt;/script&gt;在文章間的獨特程度被定義成文章總數(&lt;script type=&quot;math/tex&quot;&gt;\vert D\vert&lt;/script&gt;)，除上字詞&lt;script type=&quot;math/tex&quot;&gt;t_i&lt;/script&gt;出現的文章數(&lt;script type=&quot;math/tex&quot;&gt;\vert {j:t_i\in d_j}\vert&lt;/script&gt;)，以上面”國王”的例子來看，1000個文章裡面只出現在10個文章裡，其&lt;script type=&quot;math/tex&quot;&gt;idf=\frac{1000}{10}&lt;/script&gt;。&lt;/p&gt;

&lt;h3 id=&quot;使用的方式&quot;&gt;使用的方式&lt;/h3&gt;

&lt;p&gt;只要給定了一堆的文章，使用在上面定義的公式，就可以算出每個字詞對每篇文章的term frequency和每個字詞的inverse document frequency。&lt;/p&gt;

&lt;p&gt;當我今天輸入一些關鍵字在搜尋欄裡面時，找出相關文件的方式就是用關鍵字的inverse document frequency去乘上文章中該字詞的term frequency並加總起來，得到關鍵字和文件的相似程度。&lt;/p&gt;

&lt;p&gt;假設我今天在搜尋欄當中輸入了”&lt;script type=&quot;math/tex&quot;&gt;t_1&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;t_2&lt;/script&gt;“，與文章&lt;script type=&quot;math/tex&quot;&gt;d_j&lt;/script&gt;相似程度的算法為&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;tf\text{-}idf_{d_j}=tf_{t_1,d_j}\times idf_{t_1} + tf_{t_2,d_j}\times idf_{t_2}&lt;/script&gt;

&lt;p&gt;有了相似程度的分數就能做排序，得到與搜尋關鍵字相關的文章了。&lt;/p&gt;

&lt;h3 id=&quot;其他tf-idf的算法&quot;&gt;其他TF-IDF的算法&lt;/h3&gt;

&lt;p&gt;上面所介紹的tf-idf可以說是最基礎的算法，而有很多人嘗試用不一樣的方式來計算tf和idf，有興趣的話可以參考&lt;a href=&quot;https://en.wikipedia.org/wiki/Okapi_BM25&quot;&gt;Okapi BM25&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&quot;應用在分類上&quot;&gt;應用在分類上&lt;/h2&gt;

&lt;p&gt;前面提到了如何使用tf-idf來做一個簡易的搜尋，這邊提一下另外一個應用的方式，就是拿來做一個簡單的文字分類器。假如我們今天想要對使用者輸入進來的文字做sentiment analysis，我們手上所收集到的訓練資料如下&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;啊不就好棒棒 負面
我就爛 負面
您真厲害 正面
醒醒吧你沒有妹妹 負面
感謝乾爹 正面
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;我們想要知道使用者輸入的資料究竟是正面還是負面，做法上可以將正面和負面的label的文字全部串起來當作是一個文章&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;您真厲害感謝乾爹 （文章_正面）
啊不就好棒棒我就爛醒醒吧你沒有妹妹 （文章_負面）
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;然後各自去算文章字詞的tf和idf，接著把使用者輸入的文字當作是搜尋關鍵字，得到它對各個文章的相似度排序，便能知道輸入的文字是比較偏向正面還是比較偏向負面了，上述的做法也適用label的種類超過兩個的情況。&lt;/p&gt;</content><author><name>Your Name</name></author><category term="Machine-Learning" /><summary type="html">這篇介紹一下常被使用的TF-IDF是什麼，以及怎麼用來做一個簡單的分類器。</summary></entry><entry><title type="html">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title><link href="https://wjohn1483.github.io/#%20the%20base%20hostname%20&%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2020/03/29/BERT-pre-training-of-deep-bidirectional-transforers-for-language-understanding/" rel="alternate" type="text/html" title="BERT&amp;#58; Pre-training of Deep Bidirectional Transformers for Language Understanding" /><published>2020-03-29T00:00:00+00:00</published><updated>2020-03-29T00:00:00+00:00</updated><id>https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2020/03/29/BERT-pre-training-of-deep-bidirectional-transforers-for-language-understanding</id><content type="html" xml:base="https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2020/03/29/BERT-pre-training-of-deep-bidirectional-transforers-for-language-understanding/">&lt;p&gt;這篇介紹一下常常聽到的&lt;a href=&quot;https://arxiv.org/pdf/1810.04805.pdf&quot;&gt;BERT&lt;/a&gt;是怎麼樣運作的。&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;簡介&quot;&gt;簡介&lt;/h2&gt;

&lt;p&gt;在很多NLP的任務上常常可以聽到某個團隊用了BERT以後達到了state-of-the-art，而聽說現在在做NLP的時候都會使用BERT產生出來的embedding，再拿去給後面想要解決的NLP任務使用，得到的效果通常都還不錯，BERT成為了做各種NLP任務的起手式。&lt;/p&gt;

&lt;h2 id=&quot;方法&quot;&gt;方法&lt;/h2&gt;

&lt;p&gt;BERT的架構是前一篇&lt;a href=&quot;https://wjohn1483.github.io/2020/03/21/attention-is-all-you-need/&quot;&gt;Attention Is All You Need&lt;/a&gt;當中所提到的Transformer的encoder，只是訓練的方式和參數的設定比較不一樣，在BERT這篇paper裡面使用了兩種參數&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathrm{BERT}_{BASE}:L=12,H=768,A=12, total\ parameters=110\mathrm{M}\\\mathrm{BERT}_{LARGE}:L=24,H=1024,A=16, total\ parameters=340\mathrm{M}&lt;/script&gt;

&lt;p&gt;其中&lt;script type=&quot;math/tex&quot;&gt;L&lt;/script&gt;代表層數、&lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt;代表hidden size、&lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;代表self-attention的head數。&lt;/p&gt;

&lt;h3 id=&quot;訓練方式&quot;&gt;訓練方式&lt;/h3&gt;

&lt;h4 id=&quot;input-representation&quot;&gt;Input Representation&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;input-representation.png&quot; alt=&quot;Input Representation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;由於BERT後面所接的NLP任務可能會需要sentence embedding，或同時考慮多個sentence，所以BERT在這個地方有設計兩個特別的token，分別是&lt;code class=&quot;highlighter-rouge&quot;&gt;[CLS]&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;[SEP]&lt;/code&gt;，&lt;code class=&quot;highlighter-rouge&quot;&gt;[SEP]&lt;/code&gt;是用來分開兩個句子的，而&lt;code class=&quot;highlighter-rouge&quot;&gt;[CLS]&lt;/code&gt;是用來做classification的embedding，因為BERT是用self-attention layer為基礎，所以即便放在第一個，其輸出的embedding仍然包含整個句子的資訊。&lt;/p&gt;

&lt;p&gt;而上圖中的Segment Embeddings是為了能夠更分開兩個句子之間的差異，可以想像成&lt;script type=&quot;math/tex&quot;&gt;\mathrm{E_A}&lt;/script&gt;是一個純量等於&lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;\mathrm{E_B}&lt;/script&gt;也是一個純量等於&lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;，更詳細的解說可以參考&lt;a href=&quot;https://www.cnblogs.com/d0main/p/10447853.html&quot;&gt;這裡&lt;/a&gt;。&lt;/p&gt;

&lt;h4 id=&quot;masked-lm&quot;&gt;Masked LM&lt;/h4&gt;

&lt;p&gt;BERT在做pre-training的時候，會同時訓練兩種任務，分別是masked LM以及next sentence prediction，而masked LM做的事情有點像是克漏字，在輸入一段文字進來的時候，在每個token會有15%的機率會被替換成&lt;code class=&quot;highlighter-rouge&quot;&gt;[MASK]&lt;/code&gt;這個token，而最終希望透過一個classifier，輸入&lt;code class=&quot;highlighter-rouge&quot;&gt;[MASK]&lt;/code&gt;經過BERT得到的embedding，輸出原本被替換掉的token。&lt;/p&gt;

&lt;h4 id=&quot;next-sentence-prediction-nsp&quot;&gt;Next Sentence Prediction (NSP)&lt;/h4&gt;

&lt;p&gt;Next sentence prediction做的事情就是去判斷輸入進來的這兩個句子，是不是在順序上是連續的，像是&lt;code class=&quot;highlighter-rouge&quot;&gt;人之初&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;性本善&lt;/code&gt;就是連續的句子，但&lt;code class=&quot;highlighter-rouge&quot;&gt;人之初&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;你好嗎&lt;/code&gt;就不是連續的句子，而訓練的方式是會拿&lt;code class=&quot;highlighter-rouge&quot;&gt;[CLS]&lt;/code&gt;經過BERT得出來的embedding，來當作classifier的輸入，希望這個classifier輸出&lt;script type=&quot;math/tex&quot;&gt;\mathrm{IsNext}&lt;/script&gt;或是&lt;script type=&quot;math/tex&quot;&gt;\mathrm{NotNext}&lt;/script&gt;。&lt;/p&gt;

&lt;p&gt;底下是整體的架構圖，其實在fine-tuning的時候，會根據不一樣的任務而拿不同位置輸出的結果，細節可以參考&lt;a href=&quot;https://arxiv.org/pdf/1810.04805.pdf&quot;&gt;原本的paper&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;overall-procedure.png&quot; alt=&quot;Overall procedure&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;實驗&quot;&gt;實驗&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;test-results.png&quot; alt=&quot;Test Results&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上圖為BERT部分的實驗結果，可以看到它技壓群雄。&lt;/p&gt;

&lt;h2 id=&quot;結論&quot;&gt;結論&lt;/h2&gt;

&lt;p&gt;BERT是一個基於Transformer架構的模型，需要大量的資料來做訓練，好在作者們有提供已經訓練好的模型參數，可以直接下載下來做使用，期望使用BERT以後，能夠讓想要解決的NLP任務結果一飛沖天。&lt;/p&gt;

&lt;h2 id=&quot;參考資料&quot;&gt;參考資料&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cnblogs.com/d0main/p/10447853.html&quot;&gt;【译】为什么BERT有3个嵌入层，它们都是如何实现的&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Your Name</name></author><category term="Paper" /><category term="Natural-Language-Processing" /><summary type="html">這篇介紹一下常常聽到的BERT是怎麼樣運作的。</summary></entry><entry><title type="html">Terminal小工具</title><link href="https://wjohn1483.github.io/#%20the%20base%20hostname%20&%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2020/03/28/terminal-tools/" rel="alternate" type="text/html" title="Terminal小工具" /><published>2020-03-28T00:00:00+00:00</published><updated>2020-03-28T00:00:00+00:00</updated><id>https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2020/03/28/terminal-tools</id><content type="html" xml:base="https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2020/03/28/terminal-tools/">&lt;p&gt;這篇記錄一下我在terminal裡面常使用的一些小工具們。&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;terminal&quot;&gt;Terminal&lt;/h2&gt;

&lt;p&gt;Terminal又稱為終端機，是一個可以用指令跟電腦做溝通、操縱的介面，是個絕大多數工程師都曾使用過的東西，如果你的電腦使用的是Linux系統的話，原生就有terminal了，底下稍微介紹一下在各個系統安裝terminal的方法。&lt;/p&gt;

&lt;h3 id=&quot;cygwin&quot;&gt;Cygwin&lt;/h3&gt;

&lt;p&gt;在Windows系統上原生也有一個&lt;code class=&quot;highlighter-rouge&quot;&gt;命令提示字元&lt;/code&gt;，雖然它外表跟terminal長得蠻像的，但裡面的指令跟大多數的terminal有蠻大的差異，如果你想要在windows系統上也能夠使用terminal的話，我推薦安裝&lt;a href=&quot;https://www.cygwin.com/&quot;&gt;Cygwin&lt;/a&gt;，只需要去其網站上下載&lt;a href=&quot;https://www.cygwin.com/setup-x86_64.exe&quot;&gt;setup-x86_64&lt;/a&gt;，照著步驟做應該就能安裝好了。&lt;/p&gt;

&lt;p&gt;如果想要安裝其他套件，像是vim、git等等，可以再次執行&lt;a href=&quot;https://www.cygwin.com/setup-x86_64.exe&quot;&gt;setup-x86_64&lt;/a&gt;，並在裡面勾選，或是安裝&lt;code class=&quot;highlighter-rouge&quot;&gt;apt-cyg&lt;/code&gt;，在裡面用指令來安裝。&lt;/p&gt;

&lt;h4 id=&quot;安裝apt-cyg&quot;&gt;安裝apt-cyg&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wget https://raw.githubusercontent.com/transcode-open/apt-cyg/master/apt-cyg
&lt;span class=&quot;nb&quot;&gt;chmod&lt;/span&gt; +x apt-cyg
&lt;span class=&quot;nb&quot;&gt;mv &lt;/span&gt;apt-cyg /usr/local/bin
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;安裝完成以後就能夠使用底下的指令來安裝其他的套件了。&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apt-cyg &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;nano
apt-cyg &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;git
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;iterm&quot;&gt;iTerm&lt;/h3&gt;

&lt;p&gt;在Mac裡面也有原生的&lt;code class=&quot;highlighter-rouge&quot;&gt;終端機&lt;/code&gt;，但蠻多人都推薦在Mac上使用&lt;a href=&quot;https://www.iterm2.com/&quot;&gt;iTerm2&lt;/a&gt;，你可以使用&lt;a href=&quot;https://brew.sh/index_zh-tw.html&quot;&gt;Homebrew&lt;/a&gt;來安裝。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;安裝Homebrew&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; /bin/bash &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;curl &lt;span class=&quot;nt&quot;&gt;-fsSL&lt;/span&gt; https://raw.githubusercontent.com/Homebrew/install/master/install.sh&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;安裝iTerm2&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;brew cask &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;iterm2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;bash&quot;&gt;Bash&lt;/h2&gt;

&lt;p&gt;如果你的系統上面只能允許你使用bash的話，我自己個人會建議將底下的東西放入你的&lt;strong&gt;~/.bashrc&lt;/strong&gt;檔裡。&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;TERM&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;xterm-256color
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;PS1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;[&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\u&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\h&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\W&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\[&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;tput sgr0&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\]&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;上面的動作可以使你的terminal能顯示256色，可以透過&lt;a href=&quot;http://www.robmeerman.co.uk/_media/unix/256colors2.pl&quot;&gt;這個script&lt;/a&gt;，搭配底下的指令&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;perl 256colors2.pl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;來檢驗你的terminal是否有支援256色，於此同時將命令列調整成&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;使用者名稱@電腦名稱 當前目錄]&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;比較易於使用。&lt;/p&gt;

&lt;h2 id=&quot;zsh&quot;&gt;ZSH&lt;/h2&gt;

&lt;p&gt;若你的環境允許你安裝其他東西的話，會建議將原生的bash替換成zsh，安裝方法如下&lt;/p&gt;

&lt;h4 id=&quot;cygwin-1&quot;&gt;Cygwin&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apt-cyg &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;zsh curl git
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;iterm-1&quot;&gt;iTerm&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;brew &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;zsh zsh-completions
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;切換預設的shell&quot;&gt;切換預設的shell&lt;/h4&gt;

&lt;p&gt;若想要將預設的bash換成zsh的話，可以利用底下的指令&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;chsh &lt;span class=&quot;nt&quot;&gt;-s&lt;/span&gt; &lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;which zsh&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;並利用&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$SHELL&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;來確認是否有切換成功。&lt;/p&gt;

&lt;h3 id=&quot;oh-my-zsh&quot;&gt;oh-my-zsh&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/ohmyzsh/ohmyzsh&quot;&gt;oh-my-zsh&lt;/a&gt;是一個管理zsh的框架，提供了蠻多套件和主題可以使用，只需要打上底下的指令就能安裝好了。&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sh &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;wget &lt;span class=&quot;nt&quot;&gt;-O-&lt;/span&gt; https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;zsh-autosuggestions&quot;&gt;zsh-autosuggestions&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zsh-users/zsh-autosuggestions&quot;&gt;zsh-autosuggestions&lt;/a&gt;是一個zsh的套件，可以根據你以前打過的指令去猜測你現在要打的指令，安裝方法如下&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;下載zsh-autosuggestions&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone https://github.com/zsh-users/zsh-autosuggestions &lt;span class=&quot;nv&quot;&gt;$ZSH_CUSTOM&lt;/span&gt;/plugins/zsh-autosuggestions
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;修改&lt;strong&gt;~/.zshrc&lt;/strong&gt;，找到設定檔中&lt;code class=&quot;highlighter-rouge&quot;&gt;plugins=(git)&lt;/code&gt;的部分，將之修改成以下&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;plugins&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=(&lt;/span&gt;
     git
     zsh-autosuggestions
&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;重新載入zsh&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; ~/.zshrc
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;zsh-syntax-highlighting&quot;&gt;zsh-syntax-highlighting&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zsh-users/zsh-syntax-highlighting&quot;&gt;zsh-syntax-highlighting&lt;/a&gt;是個在zsh裡面幫你highlight一些指令、路徑等參數的套件，安裝方法如下&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;下載zsh-syntax-highlighting&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone https://github.com/zsh-users/zsh-syntax-highlighting.git &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;ZSH_CUSTOM&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:-&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;~/.oh-my-zsh/custom&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;/plugins/zsh-syntax-highlighting
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;修改&lt;strong&gt;~/.zshrc&lt;/strong&gt;，找到設定檔中&lt;code class=&quot;highlighter-rouge&quot;&gt;plugins=(git)&lt;/code&gt;的部分，將之修改成以下&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;c&quot;&gt;# If you had installed zsh-autosuggestions&lt;/span&gt;
 &lt;span class=&quot;nv&quot;&gt;plugins&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=(&lt;/span&gt;
     git
     zsh-autosuggestions
     zsh-syntax-highlighting
 &lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
 &lt;span class=&quot;c&quot;&gt;# If you only want to install zsh-syntax-highlighting&lt;/span&gt;
 &lt;span class=&quot;nv&quot;&gt;plugins&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=(&lt;/span&gt;
     git
     zsh-syntax-highlighting
 &lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;重新載入zsh&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; ~/.zshrc
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;tools&quot;&gt;Tools&lt;/h2&gt;

&lt;h3 id=&quot;fzf&quot;&gt;fzf&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/junegunn/fzf&quot;&gt;fzf&lt;/a&gt;是一個命令列的工具，使你可以fuzzy search你之前下過的指令或者是當前目錄底下的檔案，詳細的使用情形可以參考&lt;a href=&quot;https://www.youtube.com/watch?v=qgG5Jhi_Els&quot;&gt;這個影片&lt;/a&gt;，其安裝方式如下&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone &lt;span class=&quot;nt&quot;&gt;--depth&lt;/span&gt; 1 https://github.com/junegunn/fzf.git ~/.fzf
~/.fzf/install
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;在安裝完成了以後，可以隨時在命令列按下&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;Ctrl+R&amp;gt;&lt;/code&gt;來搜尋之前下過的指令、按下&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;Ctrl+T&amp;gt;&lt;/code&gt;來搜尋當前目錄底下的檔案，這個工具好用的地方是，在搜尋的時候並不需要打上完全一樣的字串，只需要打上相似的字串就可以直接幫你動態地尋找指令和命令了。&lt;/p&gt;

&lt;h3 id=&quot;hadoop-bash-completion&quot;&gt;hadoop-bash-completion&lt;/h3&gt;

&lt;p&gt;當你需要在命令列上面操作一些在hadoop filesystem上面的檔案的時候，你可能會希望hadoop也可以支援像一般bash的路徑自動完成的功能，這時你可以參考&lt;a href=&quot;https://github.com/lensesio/hadoop-bash-completion&quot;&gt;hadoop-bash-completion&lt;/a&gt;，使用的方法很簡單，只需要將repo裡面的&lt;a href=&quot;https://github.com/lensesio/hadoop-bash-completion/blob/master/hadoop-completion.sh&quot;&gt;hadoop-completion.sh&lt;/a&gt;下載下來，並source它就行了&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;source &lt;/span&gt;hadoop-completion.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;之後每當你打上hadoop相關指令或是路徑的時候，按下&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;tab&amp;gt;&lt;/code&gt;就會自動完成了。&lt;/p&gt;

&lt;h2 id=&quot;參考資料&quot;&gt;參考資料&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://dustinhsiao21.com/2019/04/09/%E9%80%8F%E9%81%8E%E5%9C%A8-mac-%E4%B8%8A%E5%AE%89%E8%A3%9Diterm2-%E6%B4%BB%E6%BD%91%E4%BD%A0%E7%9A%84%E7%B5%82%E7%AB%AF%E6%A9%9F/&quot;&gt;透過在 Mac 上安裝iTerm2 活潑你的終端機&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Your Name</name></author><category term="Tool" /><summary type="html">這篇記錄一下我在terminal裡面常使用的一些小工具們。</summary></entry><entry><title type="html">Deep contextualized word representation</title><link href="https://wjohn1483.github.io/#%20the%20base%20hostname%20&%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2020/03/23/deep-contextualized-word-representation/" rel="alternate" type="text/html" title="Deep contextualized word representation" /><published>2020-03-23T00:00:00+00:00</published><updated>2020-03-23T00:00:00+00:00</updated><id>https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2020/03/23/deep-contextualized-word-representation</id><content type="html" xml:base="https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2020/03/23/deep-contextualized-word-representation/">&lt;p&gt;這篇簡單介紹一下赫赫有名的ELMo，其源自於&lt;a href=&quot;https://www.aclweb.org/anthology/N18-1202.pdf&quot;&gt;Deep contextualized word representation&lt;/a&gt;這篇paper。
&lt;!--more--&gt;&lt;/p&gt;

&lt;h2 id=&quot;簡介&quot;&gt;簡介&lt;/h2&gt;

&lt;p&gt;在做許許多多NLP任務的時候，我們通常都會將輸入進來的字轉換成word embedding，而一個好的embedding可以使整個任務的成效更好，而這篇paper的目的就是想要去尋找好的embedding。&lt;/p&gt;

&lt;h2 id=&quot;方法&quot;&gt;方法&lt;/h2&gt;

&lt;h3 id=&quot;bidirectional-language-model&quot;&gt;Bidirectional Language Model&lt;/h3&gt;

&lt;p&gt;其主要的模型架構還蠻簡單的，就是多層的bidirectional language model。&lt;/p&gt;

&lt;p&gt;假定我們輸入一個序列&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;N=(t_1, t_2, ..., t_N)&lt;/script&gt;

&lt;p&gt;一個forward laguage model做的事情就是在給定序列的前半部&lt;script type=&quot;math/tex&quot;&gt;(t_1, t_2, ..., t_{k-1})&lt;/script&gt;，去讓&lt;script type=&quot;math/tex&quot;&gt;t_k&lt;/script&gt;被產生出來的機率可以最大，其主要的objective function如下&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(t_1, t_2, ..., t_N)=\prod \limits_{k=1}\limits^{N}p(t_k\vert t_1, t_2, ..., t_{k-1})&lt;/script&gt;

&lt;p&gt;而backward language model做的事情跟forward language model差不多，就只是將序列反著輸入進去&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(t_1, t_2, ..., t_N)=\prod \limits_{k=1}\limits^{N}p(t+k\vert t_{k+1}, t_{k+2}, ..., t_N)&lt;/script&gt;

&lt;p&gt;Birirectional language model就是將forward language model和backward language model合在一起做訓練&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum\limits_{k=1}\limits^{N}\left( \log p(t_k\vert t_1, ..., t_{k-1};\overrightarrow{\theta_{LSTM}}) + \log p(t_k\vert t_{k+1}, ..., t_N;\overleftarrow{\theta_{LSTM}}) \right)&lt;/script&gt;

&lt;h3 id=&quot;elmo&quot;&gt;ELMo&lt;/h3&gt;

&lt;p&gt;前面我們訓練多層的bidirectional language model，在每個輸入進來的token &lt;script type=&quot;math/tex&quot;&gt;t_k&lt;/script&gt;得到了很多embedding，那我們應該要用怎麼樣的方式來使用呢？這篇paper建議將這些embedding做weighted sum，而其中的weight是根據後面要解決的NLP任務來決定的。&lt;/p&gt;

&lt;p&gt;假如果們訓練了&lt;script type=&quot;math/tex&quot;&gt;L&lt;/script&gt;層bidirectional language model，我們對於每一個token &lt;script type=&quot;math/tex&quot;&gt;t_k&lt;/script&gt;可以得到&lt;script type=&quot;math/tex&quot;&gt;2L+1&lt;/script&gt;個embedding&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R_k=\left\{ x_k^{LM}, \overrightarrow{h_{k,j}^{LM}}, \overleftarrow{h_{k,j}^{LM}}\vert j=1, ..., L \right\}\\=\left\{ h_{k,j}^{LM}\vert j=0, ..., L \right\}&lt;/script&gt;

&lt;p&gt;這&lt;script type=&quot;math/tex&quot;&gt;2L+1&lt;/script&gt;個embedding包含了token一開始進來經過linear transform的embedding &lt;script type=&quot;math/tex&quot;&gt;x^{LM}_k&lt;/script&gt;，以及&lt;script type=&quot;math/tex&quot;&gt;L&lt;/script&gt;層forward/backward language model的hidden state&lt;script type=&quot;math/tex&quot;&gt;\{ \overrightarrow{h_{k,j}^{LM}}, \overleftarrow{h_{k,j}^{LM}} \}&lt;/script&gt;，而ELMo會將這&lt;script type=&quot;math/tex&quot;&gt;2L+1&lt;/script&gt;個embedding融合成一個&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathrm{ELMo}^{task}_k=E(R_k;\theta^{task})=\gamma^{task}\sum\limits_{j=0}\limits^{L}s_j^{task}h_{k,j}^{LM}&lt;/script&gt;

&lt;p&gt;其中&lt;script type=&quot;math/tex&quot;&gt;s^{task}_j&lt;/script&gt;是經過&lt;script type=&quot;math/tex&quot;&gt;\mathrm{softmax}&lt;/script&gt;標準化的weight，而&lt;script type=&quot;math/tex&quot;&gt;\gamma^{task}&lt;/script&gt;的目的是讓後面NLP的任務可以自行決定要如何去scale這整個向量。&lt;/p&gt;

&lt;h2 id=&quot;實驗&quot;&gt;實驗&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;experiments.png&quot; alt=&quot;Experiments&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以從上表中看到，使用了ELMo的embedding以後，成效就起飛了。&lt;/p&gt;

&lt;h2 id=&quot;結論&quot;&gt;結論&lt;/h2&gt;

&lt;p&gt;ELMo是個我覺得架構還蠻算簡單的，製作word embedding的方式，也可以根據你後面想要解決的NLP task做fine-tuned。&lt;/p&gt;</content><author><name>Your Name</name></author><category term="Paper" /><category term="Natural-Language-Processing" /><summary type="html">這篇簡單介紹一下赫赫有名的ELMo，其源自於Deep contextualized word representation這篇paper。</summary></entry><entry><title type="html">Attention Is All You Need</title><link href="https://wjohn1483.github.io/#%20the%20base%20hostname%20&%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2020/03/21/attention-is-all-you-need/" rel="alternate" type="text/html" title="Attention Is All You Need" /><published>2020-03-21T00:00:00+00:00</published><updated>2020-03-21T00:00:00+00:00</updated><id>https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2020/03/21/attention-is-all-you-need</id><content type="html" xml:base="https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2020/03/21/attention-is-all-you-need/">&lt;p&gt;這篇文章想要簡介一下被廣泛使用的Transformer是什麼。&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;簡介&quot;&gt;簡介&lt;/h2&gt;

&lt;p&gt;在以前處理有關sequence的問題，像是文字、語音和影像等，所使用的model不是CNN就是RNN，然而它們各自有一些缺點，像是CNN必須要疊的很深才可以看到比較長範圍的資訊，RNN比較不能應付較長的序列，並且必須sequential的做訓練，不能夠被平行化，而這篇paper提出了另一個一樣能套用在sequence上的model，稱之為Transformer。&lt;/p&gt;

&lt;h2 id=&quot;方法&quot;&gt;方法&lt;/h2&gt;

&lt;p&gt;底下的圖是paper當中所提出的Transformer的架構，跟一般做language translation的架構蠻像的，由encoder和decoder組成，但可以看到其中有一個比較特別的layer：Multi-Head Attention。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;transformer-architecture.png&quot; alt=&quot;Transformer model architecture&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;scaled-dot-product-attention&quot;&gt;Scaled Dot-Product Attention&lt;/h3&gt;

&lt;p&gt;在講Multi-Head Attention Layer之前，得先要講一下Scaled Dot-Product Attention，因為Multi-Head Attention Layer其實是由Scaled Dot-Product Attention所組成的，而下圖為Scaled Dot-Product Attention的架構。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;scaled-dot-product-attention.png&quot; alt=&quot;Scaled Dot-Product Attention&quot; /&gt;&lt;/p&gt;

&lt;p&gt;假設我們現在有一個sequence input &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{x}&lt;/script&gt;，其長度為&lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;，而每一個元&lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt;是一個&lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt;維度的向量，可以想像成是我們已經把一個輸入進來的句子轉換成了&lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt;維的word embeddings&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\boldsymbol{x}=[x_1, x_2, ..., x_n],\ x_i\in\mathbb{R}^d&lt;/script&gt;

&lt;p&gt;而圖中的&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{Q}&lt;/script&gt;、&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{K}&lt;/script&gt;、&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{V}&lt;/script&gt;，分別代表的是Query、Key、Value，是由不同的weight各自乘上輸入&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{x}&lt;/script&gt;所得到的&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\boldsymbol{Q}=[Q_1, Q_2, ..., Q_n],\ Q_i=W_Qx_i\in\mathbb{R}^{d_k}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\boldsymbol{K}=[K_1, K_2, ..., K_n],\ K_i=W_Kx_i\in\mathbb{R}^{d_k}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\boldsymbol{V}=[V_1, V_2, ..., V_n],\ V_i=W_Vx_i\in\mathbb{R}^{d_v}&lt;/script&gt;

&lt;p&gt;Scaled Dot-Product Attention做的事情就是，對每個時間點&lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;，用其Query對所有時間點的Key做內積，scale以後過&lt;script type=&quot;math/tex&quot;&gt;\mathrm{softmax}&lt;/script&gt;得到attention weight，再與Value做weighted sum。假設我們想要得到&lt;script type=&quot;math/tex&quot;&gt;t=1&lt;/script&gt;的輸出，其流程大概會是底下這樣&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;取出&lt;script type=&quot;math/tex&quot;&gt;t=1&lt;/script&gt;的query &lt;script type=&quot;math/tex&quot;&gt;Q_1&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;對所有時間點的key做內積，得到原始的attention weight &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{a}=[Q_1K_1, Q_1K_2, ..., Q_1K_n], Q_iK_j \in \mathbb{R}&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;對原始的attention weight除上維度開根號，以避免原始值太大，亦即&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{a}=\frac{\boldsymbol{a}}{\sqrt{d_k}}&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;將&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{a}&lt;/script&gt;通過&lt;script type=&quot;math/tex&quot;&gt;\mathrm{softmax}&lt;/script&gt;，得到最終的attention weight &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{\hat a}=[\hat a_1, \hat a_2, ..., \hat a_n], \hat a_i\in\mathbb{R}&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;利用attention weight對value做weighted sum，得到時間點&lt;script type=&quot;math/tex&quot;&gt;t=1&lt;/script&gt;的輸出&lt;script type=&quot;math/tex&quot;&gt;z_1&lt;/script&gt;&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;z_1=V_1*\hat a_1+V_2*\hat a_2+...+V_n*\hat a_n\in\mathbb{R}^{d_v}&lt;/script&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;其他時間點的輸出&lt;script type=&quot;math/tex&quot;&gt;z_2, ..., z_n&lt;/script&gt;都是利用相同的方式得到的，我們也可以從中發現到，其實產生某個特定時間點的輸出&lt;script type=&quot;math/tex&quot;&gt;z_t&lt;/script&gt;並沒有與其他時間點有關聯，我們可以簡單地用矩陣相乘的方式一次算出所有的輸出&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\boldsymbol{z}=[z_1, z_2, ..., z_n]=\mathrm{Attention}(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V})=\boldsymbol{V}\left(\mathrm{softmax}(\frac{\boldsymbol{Q}^T\boldsymbol{K}}{\sqrt{d_k}})\right)^T&lt;/script&gt;

&lt;p&gt;至此我們有了可以輸入一個序列&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{x}&lt;/script&gt;，得到與&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{x}&lt;/script&gt;相同長度輸出&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{z}&lt;/script&gt;的Scaled Dot-Product Attention layer。&lt;/p&gt;

&lt;h3 id=&quot;multi-head-attention-layer&quot;&gt;Multi-Head Attention Layer&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;multi-head-attention.png&quot; alt=&quot;Multi-Head Attention&quot; /&gt;&lt;/p&gt;

&lt;p&gt;而在Transformer架構中的Multi-Head Attention其實就是把輸入的&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{Q}&lt;/script&gt;、&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{K}&lt;/script&gt;、&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{V}&lt;/script&gt;過linear transform、把多個Scaled Dot-Product Attention平行的疊在一起、將每個輸出都接起來後再過一個linear transform&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathrm{MultiHead}(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V})=\mathrm{Concat}(\mathrm{head_1}, ..., \mathrm{head_h})W^O&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathrm{head_i}=\mathrm{Attention}(\boldsymbol{Q}W_i^Q, \boldsymbol{K}W_i^K, \boldsymbol{V}W_i^V)\\W_i^Q\in\mathbb{R}^{d_{model}\times d_{k}}, W_i^K\in\mathbb{R}^{d_{model}\times d_k}, W_i^V\in\mathbb{R}^{d_{model}\times d_v}&lt;/script&gt;

&lt;p&gt;在paper裡面，將&lt;script type=&quot;math/tex&quot;&gt;\mathrm{head}&lt;/script&gt;的個數設為&lt;script type=&quot;math/tex&quot;&gt;h=8&lt;/script&gt;、&lt;script type=&quot;math/tex&quot;&gt;d_k=d_v=d_{model}/h=64&lt;/script&gt;，其中的&lt;script type=&quot;math/tex&quot;&gt;d_{model}&lt;/script&gt;是Multi-Head Attention輸出的維度。&lt;/p&gt;

&lt;h3 id=&quot;add--norm&quot;&gt;Add &amp;amp; Norm&lt;/h3&gt;

&lt;p&gt;這一層做的事情是加一個residual connection，並做&lt;a href=&quot;https://arxiv.org/pdf/1607.06450.pdf&quot;&gt;Layer Normalization&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&quot;positional-encoding&quot;&gt;Positional Encoding&lt;/h3&gt;

&lt;p&gt;在上面的方法中，在產生特定時間點輸出的時候，並不會對比較近或比較遠的輸入而有不一樣的操作，使得在產生&lt;script type=&quot;math/tex&quot;&gt;z_3&lt;/script&gt;的時候，model並不知道&lt;script type=&quot;math/tex&quot;&gt;x_1&lt;/script&gt;是比較近而&lt;script type=&quot;math/tex&quot;&gt;x_{10}&lt;/script&gt;是比較遠的，因此在paper裡對輸入都加入了positional encoding，好讓model可以看輸入就知道這個輸入是哪一個時間點的輸入。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;PE(pos, 2i)=\sin(pos/10000^{2i/d_{model}})\\PE(pos, 2i+1)=\cos(pos/10000^{2i/d_{model}})&lt;/script&gt;

&lt;h3 id=&quot;encoding--decoding-動畫&quot;&gt;Encoding / Decoding 動畫&lt;/h3&gt;

&lt;p&gt;底下是從&lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;&gt;Google AI Blog&lt;/a&gt;中擷取下來的動畫，解釋了Transformer在encoding和decoding時的流程。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://3.bp.blogspot.com/-aZ3zvPiCoXM/WaiKQO7KRnI/AAAAAAAAB_8/7a1CYjp40nUg4lKpW7covGZJQAySxlg8QCLcBGAs/s640/transform20fps.gif&quot; alt=&quot;Transformer process&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;實驗&quot;&gt;實驗&lt;/h2&gt;

&lt;p&gt;Paper將Transformer應用在翻譯上，可以看到Transformer的表現超群。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;translation-results.png&quot; alt=&quot;Translation Results&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;結論&quot;&gt;結論&lt;/h2&gt;

&lt;p&gt;這篇paper提出了Transformer的架構，底層使用了Multi-Head Attention，是一個與CNN、RNN一樣，可以輸入輸出都是序列的layer，當你也有要處理序列的問題時，不妨試試看，或許可以有不錯的表現。&lt;/p&gt;</content><author><name>Your Name</name></author><category term="Paper" /><category term="Natural-Language-Processing" /><summary type="html">這篇文章想要簡介一下被廣泛使用的Transformer是什麼。</summary></entry><entry><title type="html">Vim Plugins</title><link href="https://wjohn1483.github.io/#%20the%20base%20hostname%20&%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2020/03/09/vim-plugins/" rel="alternate" type="text/html" title="Vim Plugins" /><published>2020-03-09T00:00:00+00:00</published><updated>2020-03-09T00:00:00+00:00</updated><id>https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2020/03/09/vim-plugins</id><content type="html" xml:base="https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2020/03/09/vim-plugins/">&lt;p&gt;這邊記錄一下我目前所使用的Vim plugin以及它們的功能。
&lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;市面上有很多Integrated Development Environment (IDE)來幫助你開發，像是&lt;a href=&quot;https://developer.apple.com/xcode/&quot;&gt;Xcode&lt;/a&gt;、&lt;a href=&quot;https://visualstudio.microsoft.com/zh-hant/&quot;&gt;Visual Studio&lt;/a&gt;、&lt;a href=&quot;https://www.jetbrains.com/idea/&quot;&gt;Intellij&lt;/a&gt;、&lt;a href=&quot;https://www.jetbrains.com/pycharm/&quot;&gt;Pycharm&lt;/a&gt;、&lt;a href=&quot;https://www.sublimetext.com/&quot;&gt;Sublime&lt;/a&gt;等等，然而在終端機上的文字編輯器，應該大多都是用vim來開發，而vim本身也有很多神人開發的套件，讓你可以在vim裡面做到像上面IDE一樣的操作，底下介紹一下我目前所使用的一些套件。&lt;/p&gt;

&lt;p&gt;目前我所使用的設定檔，原先是來自於&lt;a href=&quot;https://github.com/timss/vimconf&quot;&gt;timss/vimconf&lt;/a&gt;，看star數也蠻多人推薦&lt;a href=&quot;https://github.com/amix/vimrc&quot;&gt;amix/vimrc&lt;/a&gt;，你也可以去網路上找你心目中所屬的設定檔。&lt;/p&gt;

&lt;h2 id=&quot;vundle&quot;&gt;Vundle&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/VundleVim/Vundle.vim&quot;&gt;Vundle&lt;/a&gt;是一個vim套件的管理工具，使你可以直接打上vim plugin repo的名字就能安裝至你的vim上，像是&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Plug &lt;span class=&quot;s1&quot;&gt;'gmarik/Vundle.vim'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;在宣告好想要安裝的套件以後，打開vim，再打上&lt;code class=&quot;highlighter-rouge&quot;&gt;:PlugInstall&lt;/code&gt;就會將宣告的套件安裝在&lt;strong&gt;[user name]/.vim/bundle&lt;/strong&gt;裡面。如果想要移除某個套件，只需要在設定檔將套件的宣告移除，並在vim裡面打上&lt;code class=&quot;highlighter-rouge&quot;&gt;:PlugClean&lt;/code&gt;，相當的方便。&lt;/p&gt;

&lt;h2 id=&quot;plugins&quot;&gt;Plugins&lt;/h2&gt;

&lt;p&gt;接下來介紹一下我覺得好像不錯用的套件們，大多的套件只需要像底下這樣宣告，並且&lt;code class=&quot;highlighter-rouge&quot;&gt;:PlugInstall&lt;/code&gt;就能安裝完成了。&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Plug &lt;span class=&quot;s1&quot;&gt;'[plugin name]'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;ervandewsupertab&quot;&gt;&lt;a href=&quot;https://github.com/ervandew/supertab&quot;&gt;ervandew/supertab&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;一個可以讓你用tab自動完成各種東西的套件，可以用tab完成function、variable等等。&lt;/p&gt;

&lt;h3 id=&quot;ycm-coreyoucompleteme&quot;&gt;&lt;a href=&quot;https://github.com/ycm-core/YouCompleteMe&quot;&gt;ycm-core/YouCompleteMe&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;也是一個自動完成各種東西的套件，只是安裝起來沒有像supertab那樣簡單，不僅需要宣告在設定檔裡，還需要安裝其他程式語言，像是go、nodejs等，詳細的安裝流程可以參考官方的GitHub。&lt;/p&gt;

&lt;p&gt;我會在設定檔裡面多加底下的設定，使你可以按下enter就可以選擇自動完成的東西(預設好像是按&lt;code class=&quot;highlighter-rouge&quot;&gt;Ctrl + y&lt;/code&gt;)，另一個設定是讓你能用&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;Leader&amp;gt;g&lt;/code&gt;跳到function定義的部分。&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;if &lt;/span&gt;exists&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'g:plugs[&quot;YouCompleteMe&quot;]'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;let &lt;/span&gt;g:ycm_autoclose_preview_window_after_completion&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1
    &lt;span class=&quot;nb&quot;&gt;let &lt;/span&gt;g:ycm_key_list_stop_completion &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'&amp;lt;C-y&amp;gt;'&lt;/span&gt;, &lt;span class=&quot;s1&quot;&gt;'&amp;lt;CR&amp;gt;'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;
    nnoremap &amp;lt;Leader&amp;gt;g :YcmCompleter GoToDefinitionElseDeclaration&amp;lt;CR&amp;gt;
endif
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;itchynylightlinevim&quot;&gt;&lt;a href=&quot;https://github.com/itchyny/lightline.vim&quot;&gt;itchyny/lightline.vim&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;讓vim可以有status bar，讓你知道現在是處理什麼樣的檔案、在Normal mode、Visual mode還是Insert mode等。&lt;/p&gt;

&lt;h3 id=&quot;blingvim-bufferline&quot;&gt;&lt;a href=&quot;https://github.com/bling/vim-bufferline&quot;&gt;bling/vim-bufferline&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;在status bar裡面顯示目前開啟的所有檔案。&lt;/p&gt;

&lt;h3 id=&quot;mbbillundotree&quot;&gt;&lt;a href=&quot;https://github.com/mbbill/undotree&quot;&gt;mbbill/undotree&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;undotree會紀錄你對這個檔案的操作，使你可以隨時退回先前的版本，有點像簡易版的git。&lt;/p&gt;

&lt;h3 id=&quot;nanotechjellybeansvim&quot;&gt;&lt;a href=&quot;https://github.com/nanotech/jellybeans.vim&quot;&gt;nanotech/jellybeans.vim&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;vim的color schema。&lt;/p&gt;

&lt;h3 id=&quot;tomtomtcomment_vim&quot;&gt;&lt;a href=&quot;https://github.com/tomtom/tcomment_vim&quot;&gt;tomtom/tcomment_vim&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;幫助你快速將程式碼comment起來的套件，在Visual mode選定好區塊以後，打上&lt;code class=&quot;highlighter-rouge&quot;&gt;gc&lt;/code&gt;便能將選取的區塊都comment起來，更多的使用方法可以參考上方連結裡面的文件。&lt;/p&gt;

&lt;h3 id=&quot;sominivim-autoclose&quot;&gt;&lt;a href=&quot;https://github.com/Townk/vim-autoclose&quot;&gt;somini/vim-autoclose&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;自動幫你將括號或是其他該成雙成對的東西補齊的套件。&lt;/p&gt;

&lt;h3 id=&quot;tpopevim-eunuch&quot;&gt;&lt;a href=&quot;https://github.com/tpope/vim-eunuch&quot;&gt;tpope/vim-eunuch&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;讓你能在vim裡面直接使用Unix指令的套件，像是&lt;code class=&quot;highlighter-rouge&quot;&gt;:SudoEdit&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;:Rename&lt;/code&gt;等等，詳細的指令可以參考上方連結。&lt;/p&gt;

&lt;h3 id=&quot;tpopevim-fugitive&quot;&gt;&lt;a href=&quot;https://github.com/tpope/vim-fugitive&quot;&gt;tpope/vim-fugitive&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;讓你能在vim裡面直接操作git指令的套件。&lt;/p&gt;

&lt;h3 id=&quot;tpopevim-surround&quot;&gt;&lt;a href=&quot;https://github.com/tpope/vim-surround&quot;&gt;tpope/vim-surround&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;使你可以快速改變括號的套件，像是將&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&quot;Hello world!&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;迅速改成&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;'Hello world!'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;更多的指令請參考上面連結。&lt;/p&gt;

&lt;h3 id=&quot;junegunnvim-easy-align&quot;&gt;&lt;a href=&quot;https://github.com/junegunn/vim-easy-align&quot;&gt;junegunn/vim-easy-align&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;能迅速將程式碼對齊的套件，使用方式為，在Visual mode選好想對齊的區塊，打上&lt;code class=&quot;highlighter-rouge&quot;&gt;ga[分隔符號]&lt;/code&gt;啟動，還可以設定選擇要靠左、靠右對齊，詳細的使用方式可以參考上面的網址。&lt;/p&gt;

&lt;h3 id=&quot;honzavim-snippets--sirverultisnips&quot;&gt;&lt;a href=&quot;https://github.com/honza/vim-snippets&quot;&gt;honza/vim-snippets&lt;/a&gt; / &lt;a href=&quot;https://github.com/SirVer/ultisnips&quot;&gt;sirver/ultisnips&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;能快速補齊程式碼的套件，像是打上&lt;code class=&quot;highlighter-rouge&quot;&gt;def test[tab]&lt;/code&gt;就會自動幫你將function的架構打出來。&lt;/p&gt;

&lt;h3 id=&quot;mhinzvim-startify&quot;&gt;&lt;a href=&quot;https://github.com/mhinz/vim-startify&quot;&gt;mhinz/vim-startify&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;一個好看的vim開始畫面。&lt;/p&gt;

&lt;h3 id=&quot;mhinzvim-signify&quot;&gt;&lt;a href=&quot;https://github.com/mhinz/vim-signify&quot;&gt;mhinz/vim-signify&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;顯示檔案裡面有哪些部分與git上的有所差異。&lt;/p&gt;

&lt;h3 id=&quot;vim-syntasticsyntastic&quot;&gt;&lt;a href=&quot;https://github.com/vim-syntastic/syntastic&quot;&gt;vim-syntastic/syntastic&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;幫你做語法檢查的套件，我自己會加上底下這行，令&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;Leader&amp;gt;c&lt;/code&gt;當作快捷鍵。&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;noremap &amp;lt;silent&amp;gt;&amp;lt;Leader&amp;gt;c  :SyntasticCheck&amp;lt;CR&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;dense-analysisale&quot;&gt;&lt;a href=&quot;https://github.com/dense-analysis/ale&quot;&gt;dense-analysis/ale&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;也是做語法檢查的套件，與&lt;a href=&quot;#vim-syntasticsyntastic&quot;&gt;syntastic&lt;/a&gt;不同的是，ale是asynchronous的執行，所以在開啟、寫入檔案的時候不會卡住。&lt;/p&gt;

&lt;p&gt;我自己會加上底下的設定，以在各個error當中跳轉。&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;nnoremap &amp;lt;Leader&amp;gt;d :ALEDetail&amp;lt;CR&amp;gt;
nnoremap &amp;lt;Leader&amp;gt;cn :ALENext&amp;lt;CR&amp;gt;
nnoremap &amp;lt;Leader&amp;gt;cp :ALEPrevious&amp;lt;CR&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;milkypostmanvim-togglelist&quot;&gt;&lt;a href=&quot;https://github.com/milkypostman/vim-togglelist&quot;&gt;milkypostman/vim-togglelist&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;裝這個套件主要是為了配合&lt;a href=&quot;#dense-analysisale&quot;&gt;ale&lt;/a&gt;而安裝的，上面的ale會在每一行標註該行的warning和error，雖說ale可以做到把所有error都統一在一個列表裡顯示出來，但它沒有做可以toggle的指令，而這個套件就是設定快捷鍵來做這件事情。&lt;/p&gt;

&lt;p&gt;預設&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;Leader&amp;gt;l&lt;/code&gt;會toggle location list，把所有error列出來，在行數按下enter就會跳轉到那邊，&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;Leader&amp;gt;q&lt;/code&gt;會打開quickfix window。&lt;/p&gt;

&lt;h3 id=&quot;majutsushitagbar&quot;&gt;&lt;a href=&quot;https://github.com/majutsushi/tagbar&quot;&gt;majutsushi/tagbar&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;當你有使用ctags或cscope先對目錄底下的程式碼先做索引的話，可以使用tagbar在vim裡面顯示所有的tag。&lt;/p&gt;

&lt;h3 id=&quot;mileszsackvim&quot;&gt;&lt;a href=&quot;https://github.com/mileszs/ack.vim&quot;&gt;mileszs/ack.vim&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;在vim裡面使用grep或是ag(&lt;a href=&quot;https://github.com/ggreer/the_silver_searcher&quot;&gt;The Silver Searcher&lt;/a&gt;)來搜尋特定字詞的套件，我自己會放以下的設定來建立快捷鍵，打上&lt;code class=&quot;highlighter-rouge&quot;&gt;ack[space]&lt;/code&gt;會自動替換成&lt;code class=&quot;highlighter-rouge&quot;&gt;Ack!&lt;/code&gt;，在Normal mode中打上&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;Leader&amp;gt;a&lt;/code&gt;會去搜尋游標當下所在的字詞。&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;if &lt;/span&gt;executable&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'ag'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;let &lt;/span&gt;g:ackprg &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'ag --vimgrep'&lt;/span&gt;
endif
cnoreabbrev ack Ack!
nnoremap &amp;lt;Leader&amp;gt;a :Ack!&amp;lt;CR&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;scrooloosenerdtree&quot;&gt;&lt;a href=&quot;https://github.com/preservim/nerdtree&quot;&gt;scrooloose/nerdtree&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;樹狀的檔案瀏覽器。&lt;/p&gt;

&lt;h3 id=&quot;ctrlpvimctrlpvim&quot;&gt;&lt;a href=&quot;https://github.com/kien/ctrlp.vim&quot;&gt;ctrlpvim/ctrlp.vim&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;使你可以在vim裡面打上&lt;code class=&quot;highlighter-rouge&quot;&gt;Ctrl + p&lt;/code&gt;便能搜尋目錄底下的檔案名稱。&lt;/p&gt;

&lt;h3 id=&quot;junegunnfzf--junegunnfzfvim&quot;&gt;&lt;a href=&quot;https://github.com/junegunn/fzf&quot;&gt;junegunn/fzf&lt;/a&gt; / &lt;a href=&quot;https://github.com/junegunn/fzf.vim&quot;&gt;junegunn/fzf.vim&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;功能與上面的&lt;strong&gt;ctrlpvim/ctrlp.vim&lt;/strong&gt;幾乎雷同，只是底層是用fzf來做搜尋，我個人比較偏好這個套件，需要使用底下的設定來複寫&lt;code class=&quot;highlighter-rouge&quot;&gt;Ctrl + p&lt;/code&gt;的預設快捷鍵。&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;if &lt;/span&gt;exists&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'g:plugs[&quot;fzf.vim&quot;]'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    nmap &amp;lt;c-p&amp;gt; :FZF&amp;lt;CR&amp;gt;
endif
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;terrymavim-expand-region&quot;&gt;&lt;a href=&quot;https://github.com/terryma/vim-expand-region&quot;&gt;terryma/vim-expand-region&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;讓你可以透過&lt;code class=&quot;highlighter-rouge&quot;&gt;+&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;_&lt;/code&gt;來簡單的擴大或縮小選取的範圍，可以看連結內的demo。&lt;/p&gt;

&lt;h3 id=&quot;godlygeektabular&quot;&gt;&lt;a href=&quot;https://github.com/godlygeek/tabular&quot;&gt;godlygeek/tabular&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;讓你可以方便的對齊你的程式碼，可以看這邊的&lt;a href=&quot;http://vimcasts.org/episodes/aligning-text-with-tabular-vim/&quot;&gt;demo&lt;/a&gt;，我自己會加上底下的設定來建立快捷鍵。&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;if &lt;/span&gt;exists&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'g:plugs[&quot;tabular&quot;]'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    cnoreabbrev tab Tab
endif
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;其功能跟上面的&lt;strong&gt;junegunn/vim-easy-align&lt;/strong&gt;蠻相近的。&lt;/p&gt;

&lt;h3 id=&quot;terrymavim-multiple-cursors&quot;&gt;&lt;a href=&quot;https://github.com/terryma/vim-multiple-cursors&quot;&gt;terryma/vim-multiple-cursors&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;讓vim擁有像Sublime那樣的多個游標的功能。&lt;/p&gt;

&lt;h3 id=&quot;roxmavim-paste-easy&quot;&gt;&lt;a href=&quot;https://github.com/roxma/vim-paste-easy&quot;&gt;roxma/vim-paste-easy&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;在電腦上複製程式碼，想直接貼到vim裡面的時候有可能會有格式跑掉的問題，這個套件可以幫你自動在貼上前&lt;code class=&quot;highlighter-rouge&quot;&gt;set paste&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;更多資訊可以參考上面附的GitHub連結或是&lt;a href=&quot;https://vimawesome.com/plugin/vim-paste-easy&quot;&gt;這裡&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&quot;tips&quot;&gt;Tips&lt;/h2&gt;

&lt;p&gt;紀錄一下常用到的快捷鍵。&lt;/p&gt;

&lt;h3 id=&quot;查看現在所開啟檔案的相對絕對路徑&quot;&gt;查看現在所開啟檔案的相對/絕對路徑&lt;/h3&gt;

&lt;p&gt;想要查看檔案的相對路徑時，可以按下&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;Ctrl-G&amp;gt;&lt;/code&gt;，但過不久就會消失。&lt;/p&gt;

&lt;p&gt;若是想要看絕對路徑的話，按下&lt;code class=&quot;highlighter-rouge&quot;&gt;1 &amp;lt;Ctrl-G&amp;gt;&lt;/code&gt;，vim會等你按下enter以後再把路徑隱藏。&lt;/p&gt;</content><author><name>Your Name</name></author><category term="Tool" /><summary type="html">這邊記錄一下我目前所使用的Vim plugin以及它們的功能。</summary></entry><entry><title type="html">Gradient Descent和Learning Rate</title><link href="https://wjohn1483.github.io/#%20the%20base%20hostname%20&%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2020/03/07/gradient-descent-and-learning-rate/" rel="alternate" type="text/html" title="Gradient Descent和Learning Rate" /><published>2020-03-07T00:00:00+00:00</published><updated>2020-03-07T00:00:00+00:00</updated><id>https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2020/03/07/gradient-descent-and-learning-rate</id><content type="html" xml:base="https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2020/03/07/gradient-descent-and-learning-rate/">&lt;p&gt;稍微講一下Machine Learning當中經常被使用的gradient descent的概念以及調整learning rate的方法。&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;gradient-descent的概念&quot;&gt;Gradient Descent的概念&lt;/h2&gt;

&lt;p&gt;在各種Machine Learning的paper當中，常可以看到作者們都說他們是用gradient descent來找到model的參數，那gradient descent是什麼呢？&lt;/p&gt;

&lt;p&gt;假如我們今天的loss function是一個二次函數&lt;script type=&quot;math/tex&quot;&gt;\mathcal{L}(x)=x^2&lt;/script&gt;，其中&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;可以想作是model的參數，而我們的目標是希望找到一個&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;可以讓loss function可以最小。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;x-square.png&quot; alt=&quot;x square&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在訓練一開始的時候，我們會隨機產生model的參數，假設我們這次產生的參數為6，也就是&lt;script type=&quot;math/tex&quot;&gt;x=6&lt;/script&gt;，這時所得到的loss為&lt;script type=&quot;math/tex&quot;&gt;\mathcal{L}(6)=36&lt;/script&gt;，我們希望能調整一下參數&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;來讓loss下降一些，而使用的方法便是對loss function做微分，來得到目前在這個位置(&lt;script type=&quot;math/tex&quot;&gt;x=6&lt;/script&gt;)，loss function的趨勢是往哪邊走。&lt;/p&gt;

&lt;p&gt;對loss function微分並帶入現在的位置我們可以得到&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{L}'(x)=2x,\ \mathcal{L}'(6)=12&lt;/script&gt;

&lt;p&gt;意思是在&lt;script type=&quot;math/tex&quot;&gt;x=6&lt;/script&gt;的趨勢(斜率)是正的，當&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;增加的話，loss function的值也會隨之增加，反之便會減少，所以我們會將&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;去減掉其斜率來去尋找最小值，這時通常會再乘上一個人工設定的參數(learning rate) &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;，以避免一次更新參數的幅度過大，就變成了常見的gradient descent的公式&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x\leftarrow x-\alpha\bigtriangledown_x\mathcal{L}(x)&lt;/script&gt;

&lt;p&gt;假設我們這邊設定&lt;script type=&quot;math/tex&quot;&gt;\alpha=0.6&lt;/script&gt;，那經過一次更新以後，我們的參數就會變成&lt;script type=&quot;math/tex&quot;&gt;x=6-0.6*12=-1.2&lt;/script&gt;，再經過一次以後，參數會變成&lt;script type=&quot;math/tex&quot;&gt;x=-1.2-0.6*(-2.4)=0.24&lt;/script&gt;，隨著不斷的更新，參數也會越來越接近會讓loss function最小的&lt;script type=&quot;math/tex&quot;&gt;x=0&lt;/script&gt;。&lt;/p&gt;

&lt;p&gt;值得一提的是，這個人工設定的參數&lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;其實是很重要的，有可能會決定這個model會不會收斂，舉例來說，假設現在&lt;script type=&quot;math/tex&quot;&gt;x=3&lt;/script&gt;且&lt;script type=&quot;math/tex&quot;&gt;\alpha=1&lt;/script&gt;，經過一次更新以後會得到&lt;script type=&quot;math/tex&quot;&gt;x=3-1*6=-3&lt;/script&gt;，再次更新會得到&lt;script type=&quot;math/tex&quot;&gt;x=-3-1*(-6)=3&lt;/script&gt;，使得參數不斷地在&lt;script type=&quot;math/tex&quot;&gt;3&lt;/script&gt;和&lt;script type=&quot;math/tex&quot;&gt;-3&lt;/script&gt;之間震盪，不會收斂到最小值0，所以底下將會介紹一些方法來適時的調整learning rate，讓model比較有機會可以收斂到最小值。&lt;/p&gt;

&lt;h2 id=&quot;learning-rate的調整&quot;&gt;Learning Rate的調整&lt;/h2&gt;

&lt;p&gt;底下的&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;代表model的參數、&lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;為learning rate、&lt;script type=&quot;math/tex&quot;&gt;\bigtriangledown_\theta&lt;/script&gt;是對model參數作微分、&lt;script type=&quot;math/tex&quot;&gt;\mathcal{L}_\theta(x)&lt;/script&gt;是指在model參數為&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;的情況下，對model輸入&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;，loss function算出來的值。&lt;/p&gt;

&lt;h3 id=&quot;stochastic-gradient-descent&quot;&gt;Stochastic Gradient Descent&lt;/h3&gt;

&lt;p&gt;最一般的gradient descent，learning rate為人工設定的固定常數。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta\leftarrow\theta-\alpha \bigtriangledown_\theta \mathcal{L}_\theta(x)&lt;/script&gt;

&lt;p&gt;而之所以加上&lt;strong&gt;Stochastic&lt;/strong&gt;的關係是因為，在每次更新的時候是使用小批次(mini-batch)的方式，所以多了一些隨機的成份在裡面。&lt;/p&gt;

&lt;h3 id=&quot;momentum&quot;&gt;Momentum&lt;/h3&gt;

&lt;p&gt;Momentum的概念是保留前面所算出來的gradient，像是一個小球滾下山坡，並不會到一個凹槽就停住，會有從山坡上滾下所帶來的動能，而Stochastic Gradient Descent所算出來的gradient只與目前當下的參數有關，相比之下較容易卡進local minimum。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;v_t=\gamma v_{t-1}+\alpha\bigtriangledown_\theta\mathcal{L}_\theta(x)\\ \theta\leftarrow\theta-v_t&lt;/script&gt;

&lt;p&gt;這邊的&lt;script type=&quot;math/tex&quot;&gt;v_{t-1}&lt;/script&gt;可以想成是前面gradient的累積，而&lt;script type=&quot;math/tex&quot;&gt;\gamma&lt;/script&gt;是一個參數，通常會設成&lt;script type=&quot;math/tex&quot;&gt;0.9&lt;/script&gt;，讓動能(之前gradient的影響)隨著時間遞減。&lt;/p&gt;

&lt;h3 id=&quot;nesterov-accelerated-gradient-nag&quot;&gt;Nesterov Accelerated Gradient (NAG)&lt;/h3&gt;

&lt;p&gt;概念與Momentum相似，只是多預測了一步，來達到抄捷徑的效果。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;v_t=\gamma v_{t-1}+\alpha\bigtriangledown_\theta\mathcal{L}_{\theta-\gamma v_{t-1}}(x)\\ \theta\leftarrow\theta-v_t&lt;/script&gt;

&lt;p&gt;根據先前的gradient &lt;script type=&quot;math/tex&quot;&gt;v_{t-1}&lt;/script&gt;，我們可以預期model的參數&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;應該會跑到&lt;script type=&quot;math/tex&quot;&gt;\theta-\gamma v_{t-1}&lt;/script&gt;附近的地方，而在&lt;script type=&quot;math/tex&quot;&gt;\theta-\gamma v_{t-1}&lt;/script&gt;會算出新的gradient，所以我們的目標應該直接設在&lt;strong&gt;”&lt;script type=&quot;math/tex&quot;&gt;\theta-\gamma v_{t-1}&lt;/script&gt;之後要去的位置”&lt;/strong&gt;，來加速訓練的速度。&lt;/p&gt;

&lt;h3 id=&quot;adagrad&quot;&gt;Adagrad&lt;/h3&gt;

&lt;p&gt;在前面的Momentum和Nesterov Accelerated Gradient都是設好固定的learning rate，然而在訓練的過程當中通常會希望剛開始訓練時的learning rate較大，到後面調降learning rate的來找到極值，而Adagrad就是想要去動態的調整learning rate，其中的Ada是指Adaptive的意思。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta\leftarrow\theta-\frac{\alpha}{\sqrt{G_t+\epsilon}}\bigtriangledown_\theta\mathcal{L}_\theta(x)&lt;/script&gt;

&lt;p&gt;上面的&lt;script type=&quot;math/tex&quot;&gt;G_t&lt;/script&gt;指的是從第一次update到現在所有gradient的平方和，而&lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt;是避免除以0而加的一個常數，可以從式子看出來，剛開始的時候，累積的gradient很少，所以learning rate較大，而後gradient慢慢累積，learning rate就會減小了。&lt;/p&gt;

&lt;h3 id=&quot;adadelta&quot;&gt;Adadelta&lt;/h3&gt;

&lt;p&gt;雖說Adagrad可以動態的調整learning rate，但是其調整的方式只能不斷的減小，沒辦法在所有訓練的狀況下都有好的結果，而Adadelta就是想要來解決learning rate只能縮小的問題。&lt;/p&gt;

&lt;p&gt;在Adadelta的算法裡，它並不會把所有過去到現在的gradient都拿進來算平方和，而是用sliding window的方式取&lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt;個，並且在算平均時，是像Momentum那樣，使用decaying average&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E[g^2]_t=\gamma E[g^2]_{t-1}+(1-\gamma)g_t^2\\ \theta\leftarrow\theta-\frac{\alpha}{\sqrt{E[g^2]_t+\epsilon}}g_t&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;E[g^2]_t&lt;/script&gt;指的是到時間&lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;為止的gradient平方和，而&lt;script type=&quot;math/tex&quot;&gt;g_t&lt;/script&gt;是目前這個時間點&lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;的gradient，接著作者又稍微延伸了一下，試著不要設定learning rate &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\triangle \theta_t=-\frac{RMS[\triangle\theta]_{t-1}}{RMS[g]_t}g_t\\ \theta_{t+1}=\theta_t+\triangle\theta_t&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;RMS&lt;/script&gt;是root, mean, square，也就是平方相加開根號，這個式子的概念可以想成，我這次的update幅度跟之前幾次的update幅度的平均的比值來當作learning rate。&lt;/p&gt;

&lt;h3 id=&quot;rmsprop&quot;&gt;RMSprop&lt;/h3&gt;

&lt;p&gt;RMSprop好像是同時期與Adadelta發展出來的，概念與Adadelta類似，可以說是Adadelta的一個特例。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_{t+1}=\theta_t-\frac{\alpha}{\sqrt{E[g^2]_t+\epsilon}}g_t&lt;/script&gt;

&lt;h3 id=&quot;adaptive-moment-estimation-adam&quot;&gt;Adaptive Moment Estimation (Adam)&lt;/h3&gt;

&lt;p&gt;前面所介紹的Momentum會在計算參數更新時，考慮前一次更新的方向，而RMSprop會根據gradient的大小對learning rate進行調整，這邊的Adam則是兩者的集大成，將兩個東西合併在一起。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;m_t=\beta_1m_{t-1}+(1-\beta_1)g_t\\ v_t=\beta_2v_{t-1}+(1-\beta_2)g_t^2&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;m_t&lt;/script&gt;為Momentum的部分，&lt;script type=&quot;math/tex&quot;&gt;v_t&lt;/script&gt;為RMSprop的部分，而作者發現當&lt;script type=&quot;math/tex&quot;&gt;\beta_1&lt;/script&gt;和&lt;script type=&quot;math/tex&quot;&gt;\beta_2&lt;/script&gt;接近1的時候會有一些bias，所以有稍微修正一些&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat m_t=\frac{m_t}{1-\beta_1^t}\\ \hat v_t=\frac{v_t}{1-\beta_2^t}&lt;/script&gt;

&lt;p&gt;實際上更新的式子如下&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_{t+1}=\theta_t-\frac{\alpha}{\sqrt{\hat v_t}+\epsilon}\hat m_t&lt;/script&gt;

&lt;h3 id=&quot;adamax&quot;&gt;Adamax&lt;/h3&gt;

&lt;p&gt;在前面Adam裡面，調整learning rate的時候是使用&lt;script type=&quot;math/tex&quot;&gt;\mathcal{l}_2-norm&lt;/script&gt;，而Adam的作者發現&lt;script type=&quot;math/tex&quot;&gt;\mathcal{l}_{\infty}-norm&lt;/script&gt;也蠻好用的，所以嘗試放在更新的式子中&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;u_t=\beta_2^\infty v_{t-1}+(1-\beta_2^\infty)\vert g_t\vert^\infty=\max(\beta_2\cdot v_{t-1},\vert g_t\vert)\\ \theta_{t+1}=\theta_t-\frac{\alpha}{u_t}\hat m_t&lt;/script&gt;

&lt;h3 id=&quot;nadam&quot;&gt;Nadam&lt;/h3&gt;

&lt;p&gt;Adam原先所使用的Mometum為最原始版本的，而Nadam便是使用NAG版本的Momentum，詳細的推倒和公式可以看考資料中的第二個連結。&lt;/p&gt;

&lt;h2 id=&quot;參考資料&quot;&gt;參考資料&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/@chih.sheng.huang821/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E5%9F%BA%E7%A4%8E%E6%95%B8%E5%AD%B8-%E4%B8%89-%E6%A2%AF%E5%BA%A6%E6%9C%80%E4%BD%B3%E8%A7%A3%E7%9B%B8%E9%97%9C%E7%AE%97%E6%B3%95-gradient-descent-optimization-algorithms-b61ed1478bd7&quot;&gt;機器/深度學習-基礎數學(三):梯度最佳解相關算法(gradient descent optimization algorithms)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/index.html&quot;&gt;An overview of gradient descent optimization algorithms&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Your Name</name></author><category term="Machine-Learning" /><summary type="html">稍微講一下Machine Learning當中經常被使用的gradient descent的概念以及調整learning rate的方法。</summary></entry><entry><title type="html">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</title><link href="https://wjohn1483.github.io/#%20the%20base%20hostname%20&%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2020/03/06/model-agnostic-meta-learning-for-fast-adaptation-of-deep-networks/" rel="alternate" type="text/html" title="Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks" /><published>2020-03-06T00:00:00+00:00</published><updated>2020-03-06T00:00:00+00:00</updated><id>https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2020/03/06/model-agnostic-meta-learning-for-fast-adaptation-of-deep-networks</id><content type="html" xml:base="https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2020/03/06/model-agnostic-meta-learning-for-fast-adaptation-of-deep-networks/">&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1703.03400.pdf&quot;&gt;這篇paper&lt;/a&gt;想要去尋找一個最佳的初始化參數，讓model可以在各個task上都可以得到不錯的表現。&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;簡介&quot;&gt;簡介&lt;/h2&gt;

&lt;p&gt;Meta learning可以說是一個讓model學習如何學習的方式，希望讓model知道如何學習是比較有效率的，如此一來便可以使model在比較少的資料仍可以得到不錯的效果。而這篇paper提出一個叫做MAML的方法，想要嘗試找到一個很好的初始化參數，讓model用少量資料在其他task上訓練的時候，不需要太多epoch就能獲得很好的效果。&lt;/p&gt;

&lt;h2 id=&quot;方法&quot;&gt;方法&lt;/h2&gt;

&lt;h3 id=&quot;task定義&quot;&gt;Task定義&lt;/h3&gt;

&lt;p&gt;先定義一下要被學習的task。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{T}= \{ \mathcal{L}(x_1, a_1, ..., x_H, a_H), q(x_1), q(x_{t+1}|x_t, a_t), H\}&lt;/script&gt;

&lt;p&gt;其中&lt;script type=&quot;math/tex&quot;&gt;\mathcal{L}&lt;/script&gt;是loss function，&lt;script type=&quot;math/tex&quot;&gt;q(x_1)&lt;/script&gt;是初始的observation，&lt;script type=&quot;math/tex&quot;&gt;q(x_{t+1}\vert x_t, a_t)&lt;/script&gt;是transition distribution，而&lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt;是episode length。&lt;/p&gt;

&lt;p&gt;因為這篇paper並不只有做在classification的task上，也有做在reinforcement learning上，所以需要有episode的概念，而如果是classification task的話，就會令&lt;script type=&quot;math/tex&quot;&gt;H=1&lt;/script&gt;。&lt;/p&gt;

&lt;h3 id=&quot;maml&quot;&gt;MAML&lt;/h3&gt;

&lt;p&gt;作者提出的概念很直觀，我想要找到一個model初始值，讓這個初始值在各個task上面經過訓練以後都能達到不錯的效果，而我猜想是由於速度上的考量，這邊的&lt;em&gt;經過訓練&lt;/em&gt;指的是經過一次的update，所以可以將objective function寫成以下的樣子&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_i'=\theta-\alpha\bigtriangledown_\theta \mathcal{L}_{\mathcal{T}_i}(f_\theta)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min\limits_{\theta}\sum\limits_{\mathcal{T}_i \sim p(\mathcal{T})}\mathcal{L}_{\mathcal{T}_i}(f_{\theta_i'})=\sum\limits_{\mathcal{T}_i\sim p(\mathcal{T})}\mathcal{L}_{\mathcal{T}_i}(f_{\theta-\alpha\bigtriangledown_\theta\mathcal{L}_{\mathcal{T}_i}(f_\theta)})&lt;/script&gt;

&lt;p&gt;式(2)是初始參數在task &lt;script type=&quot;math/tex&quot;&gt;\mathcal{T}_i&lt;/script&gt;上經過一次update所得到的參數，式(3)是把在各個task上更新過一次的參數在各個task上的loss加總起來，來當作objective function，而實際更新初始參數的式子如下&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta\leftarrow\theta-\beta\bigtriangledown_\theta\sum\limits_{\mathcal{T}_i\sim p(\mathcal{T})}\mathcal{L}_{\mathcal{T}_i}(f_{\theta_i'})&lt;/script&gt;

&lt;p&gt;如果把其中的&lt;script type=&quot;math/tex&quot;&gt;\theta_i'&lt;/script&gt;用式(2)替換掉的話，可以發現式(4)其實有經過兩次微分，不過tensorflow也有支援兩次微分，所以在實作上可以直接照著paper的公式做出來就行。&lt;/p&gt;

&lt;h3 id=&quot;演算法&quot;&gt;演算法&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;maml-algorithm.png&quot; alt=&quot;Algorithm&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;實驗&quot;&gt;實驗&lt;/h2&gt;

&lt;p&gt;如果想對實驗了解更多的話，可以直接去&lt;a href=&quot;https://arxiv.org/pdf/1703.03400.pdf&quot;&gt;arXiv&lt;/a&gt;上面看原始的paper，這邊只貼上MAML做在classification上的結果，paper還有做在reinforcement learning上的實驗。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;classification-results.png&quot; alt=&quot;Classification results&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到MAML比起其他meta learning的方法都還要來得好一些。&lt;/p&gt;

&lt;h2 id=&quot;結論&quot;&gt;結論&lt;/h2&gt;

&lt;p&gt;作者們提出了簡單卻有效的方式找到好的初始化參數來解meta learning，而且目前deep learning的tool像是tensorflow等都有支援二次微分的計算，所以在實作上應該也不會到非常困難，在paper當中也有附上用tensorflow實作的程式碼。&lt;/p&gt;</content><author><name>Your Name</name></author><category term="Paper" /><category term="Meta-Learning" /><summary type="html">這篇paper想要去尋找一個最佳的初始化參數，讓model可以在各個task上都可以得到不錯的表現。</summary></entry></feed>