<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://wjohn1483.github.io/#%20the%20base%20hostname%20&%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://wjohn1483.github.io/#%20the%20base%20hostname%20&%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/" rel="alternate" type="text/html" /><updated>2022-05-20T05:43:05+00:00</updated><id>https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/feed.xml</id><title type="html">wjohn1483.github.io</title><subtitle></subtitle><author><name>Your Name</name></author><entry><title type="html">Learning Hierarchy-Aware Knowledge Graph Embeddings for Link Prediction</title><link href="https://wjohn1483.github.io/#%20the%20base%20hostname%20&%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2022/05/14/Learning-Hierarchy-Aware-Knowledge-Graph-Embeddings-for-Link-Prediction/" rel="alternate" type="text/html" title="Learning Hierarchy-Aware Knowledge Graph Embeddings for Link Prediction" /><published>2022-05-14T00:00:00+00:00</published><updated>2022-05-14T00:00:00+00:00</updated><id>https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2022/05/14/Learning-Hierarchy-Aware-Knowledge-Graph-Embeddings-for-Link-Prediction</id><content type="html" xml:base="https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2022/05/14/Learning-Hierarchy-Aware-Knowledge-Graph-Embeddings-for-Link-Prediction/"><![CDATA[<p>簡單記錄一下看完這篇paper的筆記。</p>

<!--more-->

<p><a href="https://arxiv.org/abs/1911.09419">這篇paper</a>是AAAI 2020中被發表的paper，相較於其他knowledge graph的paper，這篇paper把所有的entity都放進極座標當中，希望讓模型學習到越內層的entity是階層中比較高的，越外層的eneity是階層中比較低的。</p>

<h2 id="hierarchy-aware-knowledge-graph-embedding-hake">Hierarchy-Aware Knowledge Graph Embedding (HAKE)</h2>

<p><img src="./model_illustration.png" alt="Model Illustration" /></p>

<h3 id="annotation">Annotation</h3>

<p>在輸入給模型的資料當中，主要會是各個entity之間的relation，寫作<code class="language-plaintext highlighter-rouge">(head, relation, tail)</code>，指得是說<code class="language-plaintext highlighter-rouge">head</code>和<code class="language-plaintext highlighter-rouge">tail</code>之間有<code class="language-plaintext highlighter-rouge">relation</code>，而<code class="language-plaintext highlighter-rouge">head</code>是比較上層的，以上面圖片中的例子來說，可能的資料會是</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">(Device,</span><span class="w"> </span><span class="err">has_function,</span><span class="w"> </span><span class="err">Source)</span><span class="w">
</span><span class="err">(Device,</span><span class="w"> </span><span class="err">has_function,</span><span class="w"> </span><span class="err">Support)</span><span class="w">
</span><span class="err">(Source,</span><span class="w"> </span><span class="err">has_object,</span><span class="w"> </span><span class="err">Lamp)</span><span class="w">
</span><span class="err">(Source,</span><span class="w"> </span><span class="err">has_object,</span><span class="w"> </span><span class="err">Light)</span><span class="w">
</span><span class="err">...</span><span class="w">
</span></code></pre></div></div>

<p>在paper裡面我們會給每一個<code class="language-plaintext highlighter-rouge">head</code>、<code class="language-plaintext highlighter-rouge">relation</code>和<code class="language-plaintext highlighter-rouge">tail</code>各一個embedding，分別寫作$\mathbf{h}$、$\mathbf{r}$和$\mathbf{t}$，其中因為作者想要將embedding存在在極座標當中，所以每一個embedding都會存在有modulus和phase的這兩個部分，以head的embedding為例，他們分別會被寫作$\mathbf{h}_m$、$\mathbf{h}_p$。</p>

<h3 id="modulus-distance">Modulus Distance</h3>

<p>在計算兩個embedding相似性的時候，會把modulus和phase這兩個部分拆開來看，我們會希望$\mathbf{h}_m$在經過relation的轉換以後，越像$\mathbf{t}_m$越好，亦即</p>

\[\mathbf{h}_m \circ \mathbf{r}_m = \mathbf{t}_m\]

<p>而距離的部分就是看實際上跟預期的落差有多少</p>

\[d_{r,m}(\mathbf{h}_m, \mathbf{t}_m)=\left\| \mathbf{h}_m\circ\mathbf{r}_m-\mathbf{t}_m\right\|_2\]

<p>其中值得一提的是，雖然embedding本身可以有負值，但$\mathbf{r}_m$的部分會限制裡面所有的值都必須要大於零，原因是因為我們想要階層比較高的entity在接近原點的位置，由於$[\mathbf{r}_m]_i&gt;0$的特性，模型漸漸地就會將階層低的embedding往外推了。</p>

<h3 id="phase-distance">Phase Distance</h3>

<p>在phase的部分跟modulus差不多，我們希望$\mathbf{h}_p$在經過relation的轉換以後，越像$\mathbf{t}_p$越好</p>

\[(\mathbf{h}_p+\mathbf{r}_p)\mod 2\pi=\mathbf{t}_p,\ where\ \mathbf{h}_p,\mathbf{r}_p,\mathbf{t}_p\in[0,2\pi)^k\]

<p>距離上也是看兩者相差多少</p>

\[d_{r,p}(\mathbf{h}_p, \mathbf{t}_p)=\left\| \sin((\mathbf{h}_p+\mathbf{r}_p-\mathbf{t}_p)/2)\right\|_1\]

<h3 id="loss-function">Loss Function</h3>

<p>上面分別定義了modulus distance和phase distance，兩個entity實際的距離便可定義成</p>

\[d_r(\mathbf{h},\mathbf{t})=d_{r,m}(\mathbf{h}_m,\mathbf{t}_m)+\lambda d_{r,p}(\mathbf{h}_p,\mathbf{t}_p)\]

<p>其中的$\lambda$是由model自行學出的參數（$\lambda\in\mathbb{R}$），而loss function便是用self-adversarial的loss，希望positive sample的距離要小於$\gamma$，negative sample的距離要大於$\gamma$</p>

\[L=-\log\sigma(\gamma-d_r(\mathbf{h},\mathbf{t}))-\sum\limits^{n}\limits_{i=1}p(h'_i,r,t'_i)\log\sigma(d_r(\mathbf{h}'_i,\mathbf{t}'_i)-\gamma)\]

\[p(h'_j,r,y'_j\vert\left\{(h_i,r_i,t_i)\right\})=\frac{\exp\alpha f_r(\mathbf{h}'_j,\mathbf{t}'_j)}{\sum_i\exp\alpha f_r(\mathbf{h}'_i,\mathbf{t}'_i)},\ where\ \alpha\ is\ temperature\]

\[f_r(\mathbf{h},\mathbf{t})=-d_r(\mathbf{h},\mathbf{t})=-d_{r,m}(\mathbf{h},\mathbf{t})-\lambda d_{r,p}(\mathbf{h},\mathbf{t})\]

<h2 id="experiments">Experiments</h2>

<p>作者把這個HAKE模型使用在底下三個dataset上，它們的一些數據放在底下的表格中。</p>

<p><img src="./datasets.png" alt="Datasets" /></p>

<p><img src="./results.png" alt="Results" /></p>

<p>上面是與其他模型在這三個dataset上的比較，可以看到HAKE的表現不俗。</p>]]></content><author><name>Your Name</name></author><category term="Paper" /><category term="Graph-Model" /><summary type="html"><![CDATA[簡單記錄一下看完這篇paper的筆記。]]></summary></entry><entry><title type="html">Superset介紹</title><link href="https://wjohn1483.github.io/#%20the%20base%20hostname%20&%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2022/05/02/superset-introduction/" rel="alternate" type="text/html" title="Superset介紹" /><published>2022-05-02T00:00:00+00:00</published><updated>2022-05-02T00:00:00+00:00</updated><id>https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2022/05/02/superset-introduction</id><content type="html" xml:base="https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2022/05/02/superset-introduction/"><![CDATA[<p>這篇文章簡單記錄一下如何安裝superset並從Google Sheets和CSV當中匯入資料做成dashboard。</p>

<!--more-->

<h2 id="superset簡介">Superset簡介</h2>

<p>Superset是個最早由Airbnb開發，後來開源到Apache的businese intelligence工具，讓使用者可以方便地視覺化資料庫裡面的資料，而這邊的資料庫除了常見的PostgreSQL、Hive以外，還支援從Google Sheets和使用者上傳的CSV來當作資料的源頭。</p>

<h2 id="安裝superset">安裝Superset</h2>

<p>這邊放上我安裝時使用的指令，其他的安裝方法或詳細的介紹可以參考<a href="https://superset.apache.org/docs/installation/installing-superset-from-scratch">官方的安裝文件</a>。</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>yum <span class="nb">install</span> <span class="nt">-y</span> gcc gcc-c++ libffi-devel python-devel python-pip python-wheel openssl-devel cyrus-sasl-devel openldap-devel bzip2-devel xz-devel
pip3 <span class="nb">install</span> <span class="nt">--upgrade</span> pip
pip3 <span class="nb">install </span>apache-superset <span class="nv">MarkupSafe</span><span class="o">==</span>2.0.1
<span class="c"># Recompile python if is shows python cannot import bz2</span>

<span class="nb">export </span><span class="nv">FLASK_APP</span><span class="o">=</span>superset
superset db upgrade

<span class="c"># Create default roles and permissions</span>
superset init

<span class="c"># Create an admin user in your metadata database (use `admin` as username to be able to load the examples)</span>
superset fab create-admin

<span class="c"># Load some data to play with</span>
superset load_examples

<span class="c"># To start a development web server on port 8088, use -p to bind to another port</span>
superset run <span class="nt">-p</span> 8088 <span class="nt">--with-threads</span> <span class="nt">--reload</span> <span class="nt">--debugger</span>
</code></pre></div></div>

<p>在上面的指令中，我們有建立了一個admin帳號，其帳號密碼都是<code class="language-plaintext highlighter-rouge">admin</code>，並在最後一行的指令啟動了superset，這時理論上連到<code class="language-plaintext highlighter-rouge">localhost:8088</code>就能看到superset的UI了。</p>

<h2 id="製作dashboard流程">製作Dashboard流程</h2>

<p>在superset裡面要製作最後的dashboard前有幾個步驟需要先執行：</p>

<ol>
  <li>
    <p>建立Database：與database建立連線，讓superset能從database裡面撈取資料出來。</p>
  </li>
  <li>
    <p>建立Dataset：從database裡面引入table，在database裡面可能有千千萬萬個table，這邊我們需要告訴superset我們有興趣的table是哪些，只把有興趣的table schema引入到superset裡面。</p>
  </li>
  <li>
    <p>製作chart：引入了table以後，就能寫SQL或是用預先定義好的metric（$SUM(\star)$、$COUNT(\star)$）來視覺化table的資訊。</p>
  </li>
  <li>
    <p>製作dashboard：把前一個步驟製作的chart的呈現在一個dashboard中，方便一次瀏覽多個table視覺化的結果。</p>
  </li>
</ol>

<h2 id="建立database">建立Database</h2>

<p>底下介紹如何使用連結Google Sheets和PostgreSQL這兩個database，如果想要連結其他database的話，可以參考<a href="https://superset.apache.org/docs/databases/installing-database-drivers">官方文件</a>。由於superset主要是由python所編寫的，為了要能跟database做連線，我們需要安裝相關database的python API，在文件裡面有建議要安裝哪些套件。</p>

<h3 id="google-sheets">Google Sheets</h3>

<h4 id="安裝driver">安裝driver</h4>

<p>因為在 前面是透過<code class="language-plaintext highlighter-rouge">pip3 install</code>來安裝superset的，所以在這邊也只需要簡單的安裝pip package就能讓superset使用了，不過有可能會需要重新啟動superset來讓它吃到最新的API。</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip3 <span class="nb">install </span>shillelagh
</code></pre></div></div>

<h4 id="引入database">引入database</h4>

<p>在安裝好Google Sheets driver以後，在superset的UI上面點選<code class="language-plaintext highlighter-rouge">Data&gt;Databases</code>，在右邊可以看到新增Database的按鈕，點下去以後理論上就能看到<code class="language-plaintext highlighter-rouge">Google Sheets</code>的選項了。</p>

<p><img src="./supported_databases_google_sheets.png" alt="Supported Databases - Google Sheets" /></p>

<p>在下一個畫面當中只要填上相對應的名稱和網址就能順利建立了，需要注意的是Google Sheets的權限必須要設定成所有人都能觀看才能引入，如果想要讓私人的Google Sheets能被superset讀取的話，可以參考<a href="https://docs.preset.io/docs/google-sheets-private-connection">這篇文章</a>，另外Google Sheets裡面每一個不同的tab都需要手動新增進來，假如說我們想要引入這個sheets裡面的<code class="language-plaintext highlighter-rouge">Simple sheet</code>、<code class="language-plaintext highlighter-rouge">2 header sheet</code>和<code class="language-plaintext highlighter-rouge">birth_names</code>的話，就會需要填寫以下的設定。</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Name: simple sheet
URL: https://docs.google.com/spreadsheets/d/1_rN3lm0R_bU3NemO0s9pbFkY5LQPcuy1pscv8ZXPtg8/edit#gid<span class="o">=</span>0

Name: 2 header sheet
URL: https://docs.google.com/spreadsheets/d/1_rN3lm0R_bU3NemO0s9pbFkY5LQPcuy1pscv8ZXPtg8/edit#gid<span class="o">=</span>1077884006

Name: birth names
URL: https://docs.google.com/spreadsheets/d/1_rN3lm0R_bU3NemO0s9pbFkY5LQPcuy1pscv8ZXPtg8/edit#gid<span class="o">=</span>174770703
</code></pre></div></div>

<h3 id="postgresql">PostgreSQL</h3>

<h4 id="安裝postgresql">安裝PostgreSQL</h4>

<p>安裝PostgreSQL的方式在不同的OS上不太一樣，這邊貼上我安裝時使用的指令。</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>yum <span class="nb">install</span> <span class="nt">-y</span> https://download.postgresql.org/pub/repos/yum/reporpms/EL-7-x86_64/pgdg-redhat-repo-latest.noarch.rpm
<span class="nb">sudo </span>yum <span class="nb">install</span> <span class="nt">-y</span> postgresql14-server
<span class="nb">sudo</span> /usr/pgsql-14/bin/postgresql-14-setup initdb
<span class="nb">sudo </span>systemctl <span class="nb">enable </span>postgresql-14
<span class="nb">sudo </span>systemctl start postgresql-14
</code></pre></div></div>

<p>安裝好PostgreSQL以後，我們需要在其中建立使用者帳號，可以參考底下的指令，記得將<code class="language-plaintext highlighter-rouge">USER_NAME</code>替換成自己的帳號名稱。</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo</span> <span class="nt">-i</span> <span class="nt">-u</span> postgres
createuser USER_NAME
createdb USER_NAME
</code></pre></div></div>

<p>如果順利的話，理論上在自己帳號底下執行<code class="language-plaintext highlighter-rouge">psql</code>應該就能進入PostgreSQL的介面了，這時我們需要幫這個帳號設定一個密碼方便superset來登入。</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># In psql command line</span>
alter user USER_NAME password <span class="s1">'PASSWORD'</span><span class="p">;</span>
</code></pre></div></div>

<h4 id="安裝driver-1">安裝driver</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip3 <span class="nb">install </span>psycopg2-binary
</code></pre></div></div>

<h4 id="引入database-1">引入database</h4>

<p>安裝好driver以後，在按下新增database的按鈕時，應該就能看到PostgreSQL的選項了，這邊貼上預設的設定，理論上把<code class="language-plaintext highlighter-rouge">USER_NAME</code>、<code class="language-plaintext highlighter-rouge">PASSWORD</code>替換成自行設定的值後按下connect就可以了。</p>

<p><img class="image image--xl" src="./postgresql_database_settings.png" /></p>

<p>如果希望讓superset支援CSV上傳的功能，需要在<code class="language-plaintext highlighter-rouge">ADVANCED</code>的設定中的<code class="language-plaintext highlighter-rouge">Security</code>裡面勾選<code class="language-plaintext highlighter-rouge">Allow data upload</code>的選項，上傳的CSV會在PostgreSQL裡面建立一張新的table。</p>

<h2 id="建立dataset">建立Dataset</h2>

<p>在成功引入database以後，在<code class="language-plaintext highlighter-rouge">Data&gt;Datasets</code>裡面按下新增Dataset的按鈕，選擇好database就能看到前面引入的database裡面的table們了，點選<code class="language-plaintext highlighter-rouge">ADD</code>以後就能在chart裡面讀取這些table的資料。</p>

<p><img class="image image--xl" src="./add_dataset.png" /></p>

<h2 id="製作chart">製作Chart</h2>

<p>成功引入dataset以後，在create chart的部分就能找到先前引入的dataset了，接著就能根據想看的資訊來做出漂亮的圖表。</p>

<p><img src="./create_new_chart.png" alt="Create New Chart" /></p>

<p><img src="./chart_settings.png" alt="Chart Settings" /></p>

<h2 id="製作dashboard">製作Dashboard</h2>

<p>在把table的資訊視覺化成chart以後，如果想要將多個chart顯示在同一個畫面可以使用dashboard，在建立dashboard的畫面裡面可以用拖曳的方式來把想要顯示的chart放進來。</p>

<p><img src="./dashboard_settings.png" alt="Dashboard Settings" /></p>

<p>最後按下Save就大功告成了。</p>

<p><img src="./dashboard_demo.png" alt="Dashboard Demo" /></p>]]></content><author><name>Your Name</name></author><category term="Tool" /><summary type="html"><![CDATA[這篇文章簡單記錄一下如何安裝superset並從Google Sheets和CSV當中匯入資料做成dashboard。]]></summary></entry><entry><title type="html">Airflow介紹</title><link href="https://wjohn1483.github.io/#%20the%20base%20hostname%20&%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2022/02/26/airflow-introduction/" rel="alternate" type="text/html" title="Airflow介紹" /><published>2022-02-26T00:00:00+00:00</published><updated>2022-02-26T00:00:00+00:00</updated><id>https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2022/02/26/airflow-introduction</id><content type="html" xml:base="https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2022/02/26/airflow-introduction/"><![CDATA[<p>這篇文章紀錄一下嘗試使用Airflow的經歷。</p>

<!--more-->

<p>之前在聽一些技術分享的時候，有聽到別人使用Airflow來作為他們的排程系統，剛好在網路上也有看到別人<a href="https://leemeng.tw/a-story-about-airflow-and-data-engineering-using-how-to-use-python-to-catch-up-with-latest-comics-as-an-example.html">使用Airflow來追漫畫連載</a>，想說我也來做一個簡單的每天登入領獎勵的程式來學習Airflow怎麼使用，底下紀錄一下製作的過程。</p>

<h2 id="安裝airflow">安裝Airflow</h2>

<p><a href="https://airflow.apache.org/">Airflow</a>是由Airbnb開發的排程系統，主要是由python編寫，安裝起來也相當地方便，只需要執行底下的指令就行。</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip3 <span class="nb">install</span> <span class="s2">"apache-airflow[crypto, slack]"</span>
</code></pre></div></div>

<h3 id="初始化database">初始化database</h3>

<p>在安裝完成以後，會需要先初始化airflow的database，這個database會用來儲存任務執行的設定和log。</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># export AIRFLOW_HOME=/path/you/want</span>
airflow db init
</code></pre></div></div>

<p>預設的路徑會放在<code class="language-plaintext highlighter-rouge">~/airflow</code>，你也可以藉由<code class="language-plaintext highlighter-rouge">export AIRFLOW_HOME</code>來選擇自己喜歡的位置。</p>

<h3 id="開啟web-server">開啟web server</h3>

<p>接下來就可以準備啟動airflow了，我們可以使用底下的指令開啟web server，並設定port為8080。</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>airflow webserver <span class="nt">-p</span> 8080
</code></pre></div></div>

<p>這時就可以連去<code class="language-plaintext highlighter-rouge">localhost:8080</code>來看到airflow的使用者介面了，但進去的時候會發現要帳號密碼才能登入，我們可以透過下面的指令來創建一個權限為admin的帳戶，其帳號密碼都是<code class="language-plaintext highlighter-rouge">admin</code>。</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>airflow <span class="nb">users </span>create <span class="nt">--role</span> Admin <span class="nt">--username</span> admin <span class="nt">--email</span> admin <span class="nt">--firstname</span> admin <span class="nt">--lastname</span> admin <span class="nt">--password</span> admin
</code></pre></div></div>

<p>理論上在這個步驟做完以後就能順利地看到類似於底下的畫面了。</p>

<p><img src="./airflow_ui.png" alt="Airflow UI" /></p>

<h3 id="開啟scheduler">開啟scheduler</h3>

<p>雖說UI已經打得開了，但會發現上面的任務都沒辦法執行，原因是因為實際上排程這些任務的是scheduler，而web server就真的只是一個UI讓使用者方便使用，所以我們還需要執行底下的指令才能啟動。</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>airflow scheduler
</code></pre></div></div>

<p>scheduler和web server的關係可以參考<a href="https://airflow.apache.org/docs/apache-airflow/stable/concepts/overview.html">airflow官方文件</a>裡面的架構圖。</p>

<p><img src="https://airflow.apache.org/docs/apache-airflow/stable/_images/arch-diag-basic.png" alt="Airflow Architecture" /></p>

<h2 id="建立屬於自己的任務">建立屬於自己的任務</h2>

<p>前面有提到airflow主要是由python來編寫的，當我們要加入一個新的任務時，我們也是寫一個python的script，並放入<code class="language-plaintext highlighter-rouge">${AIRFLOW_HOME}/dags/</code>（預設是<code class="language-plaintext highlighter-rouge">~/airflow/dags/</code>）這個路徑底下，airflow就會自動抓取你寫的script，在UI上面顯示出來。</p>

<h3 id="airflow-python-script">Airflow Python Script</h3>

<p>一個簡單的任務可以參考底下的程式碼。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span><span class="p">,</span> <span class="n">timedelta</span>

<span class="kn">from</span> <span class="nn">airflow</span> <span class="kn">import</span> <span class="n">DAG</span>
<span class="kn">from</span> <span class="nn">airflow.operators.python_operator</span> <span class="kn">import</span> <span class="n">PythonOperator</span>
<span class="kn">from</span> <span class="nn">airflow.operators.dummy_operator</span> <span class="kn">import</span> <span class="n">DummyOperator</span>


<span class="k">def</span> <span class="nf">login_func</span><span class="p">(</span><span class="n">account</span><span class="p">,</span> <span class="n">password</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Attempt to login with account = </span><span class="si">{</span><span class="n">account</span><span class="si">}</span><span class="s">, password = </span><span class="si">{</span><span class="n">password</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>


<span class="n">account</span> <span class="o">=</span> <span class="s">"user"</span>
<span class="n">password</span> <span class="o">=</span> <span class="s">"password"</span>
<span class="n">default_args</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">"owner"</span><span class="p">:</span> <span class="s">"THE_ONE"</span><span class="p">,</span>
    <span class="s">"start_date"</span><span class="p">:</span> <span class="n">datetime</span><span class="p">(</span><span class="mi">2022</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
    <span class="s">"schedule_interval"</span><span class="p">:</span> <span class="s">"@daily"</span><span class="p">,</span>
    <span class="s">"retries"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="s">"retry_delay"</span><span class="p">:</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">minutes</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="p">}</span>

<span class="k">with</span> <span class="n">DAG</span><span class="p">(</span><span class="s">"login_dag"</span><span class="p">,</span> <span class="n">default_args</span><span class="o">=</span><span class="n">default_args</span><span class="p">)</span> <span class="k">as</span> <span class="n">dag</span><span class="p">:</span>
    <span class="n">login</span> <span class="o">=</span> <span class="n">PythonOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span><span class="s">"login"</span><span class="p">,</span>
        <span class="n">python_callable</span><span class="o">=</span><span class="n">login_func</span><span class="p">,</span>
        <span class="n">op_args</span><span class="o">=</span><span class="p">[</span><span class="n">account</span><span class="p">,</span> <span class="n">password</span><span class="p">],</span>
        <span class="n">provide_context</span><span class="o">=</span><span class="bp">True</span>
    <span class="p">)</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">DummyOperator</span><span class="p">(</span><span class="n">task_id</span><span class="o">=</span><span class="s">"do_nothing"</span><span class="p">)</span>

    <span class="n">login</span> <span class="o">&gt;&gt;</span> <span class="n">end</span>
</code></pre></div></div>

<p>在上面的程式碼裡面，我們先定義了一個function，假裝是要登入某個服務，接著定義了function需要使用到的參數<code class="language-plaintext highlighter-rouge">account</code>和<code class="language-plaintext highlighter-rouge">password</code>，再來是這個任務的參數，最後是整個任務的流程。</p>

<p><a href="https://airflow.apache.org/docs/apache-airflow/stable/concepts/operators.html">airflow提供了許許多多的operator</a>，可以根據自己的需要來去選擇哪一個operator比較適合這個任務，在定義好了operator的object以後，再透過<code class="language-plaintext highlighter-rouge">login &gt;&gt; end</code>這樣的方式來去把這些operator串在一起，這邊也支援分支，只要整個流程是一個Directed Acyclic Graph（DAG）就行。</p>

<p>值得一提的是，我在寫的時候如果把<code class="language-plaintext highlighter-rouge">with DAG(...):</code>這一段放進<code class="language-plaintext highlighter-rouge">if __name__ == "__main__"</code>的話，好像airflow就抓不到的樣子，在寫的時候需要注意一下。</p>

<h3 id="測試新加入的任務">測試新加入的任務</h3>

<p>在寫好上面的script以後，可以直接像以往寫python的方式那樣去執行看看，它並不會真的去執行，airflow會去看有沒有哪邊有語法上的錯誤。</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 /path/to/your/script.py
</code></pre></div></div>

<p>如果想要實際測試看看，可以上去UI、點進job裡面，右上角有個三角形可以手動跑整個流程試試看。</p>

<p><img src="./airflow_job.png" alt="Airflow Job Page" /></p>

<p>如果跑起來有符合預期的話，就能toggle左上角的選項，啟動這個DAG，讓它根據設定的頻率來執行了。</p>

<p>值得一提的是，當開啟這個DAG以後，airflow會補跑<code class="language-plaintext highlighter-rouge">start_date</code>到今天的所有job，所以要先確定一下<code class="language-plaintext highlighter-rouge">start_date</code>有沒有設定對再開啟會比較好。</p>

<h3 id="開始的時間跟預期的不同">開始的時間跟預期的不同？</h3>

<p>在上面的script裡面，我們設定</p>

<pre><code class="language-python3">    "start_date": datetime(2022, 2, 8, 11, 0),
    "schedule_interval": "@daily",
</code></pre>

<p>期望它可以從2022/2/8的早上11點開始執行第一次，並以天為頻率來執行，但實際上會發現它第一次的執行時間會是2022/2/9的早上11點，原因是因為上面寫的是任務時間，跟實際上執行的時間不同，airflow會在<code class="language-plaintext highlighter-rouge">start_date+schedule_interval</code>的時間過完以後才開始處理<code class="language-plaintext highlighter-rouge">start_date+schedule_interval</code>的資料。</p>

<p>想法上有點類似於，我想將今天使用者的資料統整起來，並壓上今天的日期，但我實際執行統整這個動作的時間會是在明天，因為今天還沒有過完，如果今天就做統整，就會有部分的資料被漏掉。</p>

<h2 id="結論">結論</h2>

<p>Airflow是個安裝簡單、功能也很齊全的排程系統，上面敘述的部分只是airflow的九牛一毛而已，還有許許多多的功能沒有被覆蓋到，如果有興趣或是有複雜的功能想要實現，可以去看看<a href="https://airflow.apache.org/docs/apache-airflow/stable/index.html">airflow的官方文件</a>。</p>

<h2 id="參考資料">參考資料</h2>

<ol>
  <li><a href="https://leemeng.tw/a-story-about-airflow-and-data-engineering-using-how-to-use-python-to-catch-up-with-latest-comics-as-an-example.html">一段 Airflow 與資料工程的故事：談如何用 Python 追漫畫連載</a></li>
</ol>]]></content><author><name>Your Name</name></author><category term="Tool" /><summary type="html"><![CDATA[這篇文章紀錄一下嘗試使用Airflow的經歷。]]></summary></entry><entry><title type="html">AutoKeras介紹</title><link href="https://wjohn1483.github.io/#%20the%20base%20hostname%20&%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2021/12/23/autokeras-introduction/" rel="alternate" type="text/html" title="AutoKeras介紹" /><published>2021-12-23T00:00:00+00:00</published><updated>2021-12-23T00:00:00+00:00</updated><id>https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2021/12/23/autokeras-introduction</id><content type="html" xml:base="https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2021/12/23/autokeras-introduction/"><![CDATA[<p>現在市面上有眾多AutoML的框架，而用深度學習並有Neural Architecture Search（NAS）功能的並不多，這邊紀錄一下使用AutoKeras的心得。</p>

<!--more-->

<h2 id="安裝autokeras">安裝AutoKeras</h2>

<p>安裝的方式很簡單，只需要<code class="language-plaintext highlighter-rouge">pip install</code>就行了，不過值得一提的是，在文章撰寫的當下，雖然<a href="https://autokeras.com/install/">AutoKeras的官網</a>上寫支援tensorflow 2.3.0以上的版本，但實際用tensorflow 2.7.0的時候會出現問題，建議還是先使用tensorflow 2.3.0。</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip3 <span class="nb">install </span><span class="nv">tensorflow</span><span class="o">==</span>2.3.0 autokeras
</code></pre></div></div>

<h2 id="利用autokeras做text-classification">利用AutoKeras做text classification</h2>

<p>在<a href="https://autokeras.com/tutorial/overview/">AutoKeras的官網</a>上有很多tutorial來做不同的任務，像是text classification、image classification等等，這邊嘗試的是tutorial裡面的text classification，任務是sentiment analysis，給定一個評論，判斷這個評論是正面還是負面的。</p>

<h3 id="準備訓練用的資料">準備訓練用的資料</h3>

<p>底下的程式碼會去從網路上抓取dataset下來，並做成numpy，值得一提的是，文字的部分我們並沒有轉成index，而是單純的string，轉成index的部分會交由autokeras放在模型裡面。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_files</span>


<span class="k">print</span><span class="p">(</span><span class="s">"Preparing data..."</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">get_file</span><span class="p">(</span>
    <span class="n">fname</span><span class="o">=</span><span class="s">"aclImdb.tar.gz"</span><span class="p">,</span>
    <span class="n">origin</span><span class="o">=</span><span class="s">"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"</span><span class="p">,</span>
    <span class="n">extract</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># set path to dataset
</span><span class="n">IMDB_DATADIR</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">dataset</span><span class="p">),</span> <span class="s">"aclImdb"</span><span class="p">)</span>

<span class="n">classes</span> <span class="o">=</span> <span class="p">[</span><span class="s">"pos"</span><span class="p">,</span> <span class="s">"neg"</span><span class="p">]</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">load_files</span><span class="p">(</span>
    <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">IMDB_DATADIR</span><span class="p">,</span> <span class="s">"train"</span><span class="p">),</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">categories</span><span class="o">=</span><span class="n">classes</span>
<span class="p">)</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">load_files</span><span class="p">(</span>
    <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">IMDB_DATADIR</span><span class="p">,</span> <span class="s">"test"</span><span class="p">),</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">categories</span><span class="o">=</span><span class="n">classes</span>
<span class="p">)</span>

<span class="n">x_train</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">train_data</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">train_data</span><span class="p">.</span><span class="n">target</span><span class="p">)</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">test_data</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">test_data</span><span class="p">.</span><span class="n">target</span><span class="p">)</span>

<span class="c1"># Minimize training size for tutorial
</span><span class="n">sample_size</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[:</span><span class="n">sample_size</span><span class="p">]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[:</span><span class="n">sample_size</span><span class="p">]</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Data samples..."</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (sample_size,)
</span><span class="k">print</span><span class="p">(</span><span class="n">y_train</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (sample_size,)
</span><span class="k">print</span><span class="p">(</span><span class="n">x_train</span><span class="p">[</span><span class="mi">0</span><span class="p">][:</span><span class="mi">50</span><span class="p">])</span>  <span class="c1"># b'Zero Day leads you to think, even re-think why two'
</span><span class="k">print</span><span class="p">(</span><span class="n">y_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">30</span><span class="p">])</span>  <span class="c1"># [1 0 1 0 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0]
</span></code></pre></div></div>

<h3 id="訓練模型">訓練模型</h3>

<p>在這邊我們使用autokeras裡面的TextClassifier，設定<code class="language-plaintext highlighter-rouge">max_trials</code>為2，代表嘗試2種不同的模型架構，可以視情況把這個數字調大，而<code class="language-plaintext highlighter-rouge">overwrite</code>會將上次訓練的結果覆蓋掉，如果想接續上次訓練，可以改成<code class="language-plaintext highlighter-rouge">False</code>。</p>

<p>在<code class="language-plaintext highlighter-rouge">fit()</code>裡面有設定<code class="language-plaintext highlighter-rouge">epochs</code>，表示每一個模型架構會訓練多少個epoch。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">autokeras</span> <span class="k">as</span> <span class="n">ak</span>


<span class="k">print</span><span class="p">(</span><span class="s">"Building model..."</span><span class="p">)</span>
<span class="c1"># Initialize the text classifier.
</span><span class="n">clf</span> <span class="o">=</span> <span class="n">ak</span><span class="p">.</span><span class="n">TextClassifier</span><span class="p">(</span>
    <span class="n">overwrite</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">max_trials</span><span class="o">=</span><span class="mi">2</span>
<span class="p">)</span>  <span class="c1"># It only tries 2 models as a quick demo.
</span><span class="n">clf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div></div>

<p>值得一提的是，<code class="language-plaintext highlighter-rouge">TextClassifier</code>可以設定模型的metrics要是什麼，預設是<code class="language-plaintext highlighter-rouge">val_loss</code>，可以依據任務的需求來做修改。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cls</span> <span class="o">=</span> <span class="n">ak</span><span class="p">.</span><span class="n">TextClassifier</span><span class="p">(</span>
    <span class="n">overwrite</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">max_trials</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">objective</span><span class="o">=</span><span class="s">"accuracy"</span><span class="p">,</span>  <span class="c1"># Change to other metric that is suitable for your task
</span><span class="p">)</span>
</code></pre></div></div>

<p>另外，<code class="language-plaintext highlighter-rouge">TextClassifier</code>會自己去判斷label有多少個，並自己對label做index，只是在預測的時候並不會自動把index轉回原本label的樣子，建議這邊自己先用<code class="language-plaintext highlighter-rouge">sklearn.preprocessing.LabelEncoder</code>先對label做index，並在<code class="language-plaintext highlighter-rouge">fit()</code>的時候餵入轉好index的資料，之後在預測時，就可以使用<code class="language-plaintext highlighter-rouge">LabelEncoder</code>來轉回label原本的樣子。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pickle</span>


<span class="c1"># Before training
</span><span class="n">labeler</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">().</span><span class="n">fit</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
<span class="n">label</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">labeler</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">label</span><span class="p">))</span>
<span class="n">pickle</span><span class="p">.</span><span class="n">dump</span><span class="p">(</span><span class="n">labeler</span><span class="p">,</span> <span class="nb">open</span><span class="p">(</span><span class="s">"/path/to/labeler.pkl"</span><span class="p">,</span> <span class="s">"wb"</span><span class="p">),</span> <span class="n">pickle</span><span class="p">.</span><span class="n">HIGHEST_PROTOCOL</span><span class="p">)</span>

<span class="c1"># After training
</span><span class="n">labeler</span> <span class="o">=</span> <span class="n">pickle</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s">"/path/to/labeler.pkl"</span><span class="p">,</span> <span class="s">"rb"</span><span class="p">))</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">labeler</span><span class="p">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div></div>

<h3 id="儲存模型">儲存模型</h3>

<p>這邊會儲存表現最好的模型。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="n">export_model</span><span class="p">()</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">model</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">"./model"</span><span class="p">,</span> <span class="n">save_format</span><span class="o">=</span><span class="s">"tf"</span><span class="p">)</span>
<span class="k">except</span> <span class="nb">Exception</span><span class="p">:</span>
    <span class="n">model</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">"./model.h5"</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="模型預測">模型預測</h3>

<p>autokeras輸出的模型跟一般keras訓練出來的模型相同，所以我們可以用keras的<code class="language-plaintext highlighter-rouge">load_model()</code>來讀取模型，不過會需要在後面加上<code class="language-plaintext highlighter-rouge">custom_objects</code>，把autokeras自定義的object帶進來。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">load_model</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="s">"./model"</span><span class="p">,</span> <span class="n">custom_objects</span><span class="o">=</span><span class="n">ak</span><span class="p">.</span><span class="n">CUSTOM_OBJECTS</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">summary</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="結論">結論</h2>

<p>可以稍微瀏覽一下<a href="https://autokeras.com/tutorial/overview/">autokeras的tutorial</a>，如果碰到的任務種類有在裡面，可以嘗試看看，只不過會需要跑在可以連上外網的機器上，因為autokeras會去網路上抓一些pretrain的模型下來，另外也需要注意一下硬碟的使用量，因為autokeras會把每個trial訓練的模型儲存下來，很容易把硬碟吃滿，不然就是需要在<a href="https://autokeras.com/text_classifier/">ak.TextClassifier</a>裡面限制<code class="language-plaintext highlighter-rouge">max_model_size</code>的大小。</p>]]></content><author><name>Your Name</name></author><category term="Tool" /><summary type="html"><![CDATA[現在市面上有眾多AutoML的框架，而用深度學習並有Neural Architecture Search（NAS）功能的並不多，這邊紀錄一下使用AutoKeras的心得。]]></summary></entry><entry><title type="html">Unsupervised Speech Recognition</title><link href="https://wjohn1483.github.io/#%20the%20base%20hostname%20&%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2021/08/28/unsupervised-speech-recognition/" rel="alternate" type="text/html" title="Unsupervised Speech Recognition" /><published>2021-08-28T00:00:00+00:00</published><updated>2021-08-28T00:00:00+00:00</updated><id>https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2021/08/28/unsupervised-speech-recognition</id><content type="html" xml:base="https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2021/08/28/unsupervised-speech-recognition/"><![CDATA[<p>簡單介紹一下Facebook AI Research前陣子出的、以非監督的方式來做語音辨識的<a href="https://ai.facebook.com/research/publications/unsupervised-speech-recognition">paper</a>。</p>

<!--more-->

<h2 id="model-framework">Model Framework</h2>

<p>在這篇<a href="https://ai.facebook.com/blog/wav2vec-unsupervised-speech-recognition-without-supervision/">官方的部落格文章</a>裡面有對整篇paper有個大致的介紹，也有一段影片講解這篇paper使用的方法。</p>

<p><img src="./wav2vec_u_framework.png" alt="wav2vec U Framework" /></p>

<p>上圖是這篇paper wav2vec U的整體架構，首先會先把整段語音透過wav2vec 2.0轉換成feature sequence，再來對這些feature做k-means，藉由k-means所獲得的cluster來對語音訊號做segmentation，最後將segment好的語音訊號的feature輸入至generator中轉換成phoneme sequence，搭配GAN的方式來去做訓練。</p>

<h2 id="wav2vec-20">wav2vec 2.0</h2>

<p><img src="./wav2vec_2_framework.png" alt="wav2vec 2.0 Framework" /></p>

<p>wav2vec 2.0的訓練方式跟BERT有些類似，首先會先將語音訊號透過CNN抽取出這段語音的特徵，接著對它做product quantization，在把quantization前的feature輸入進transformer以後，希望在被mask掉的位置所產生出來的feature要越像quantization後的feature越好，在訓練的時候也會拿其他時間點經過quantization後的feature來做contrasive training。</p>

<h3 id="product-quantization">Product Quantization</h3>

<p><img src="https://i.typlog.com/fabwrite/NM/ZL0fPpntjxi3pzrQirdw.png?x-oss-process=style/l" alt="Product Quantization" /></p>

<p><em><a href="http://www.fabwrite.com/productquantization">实例理解product quantization算法</a></em></p>

<p>假如說我們現在有5萬張圖片的feature，每個feature有1024維，我們將這1024維切成8份，每份有128維如上圖那樣，接著以維度為單位來做k-means，這邊令k=256，我們就可以將原先1024維的feature轉換成由8個cluster id所組成的向量，把1024維降成了8維。</p>

<h2 id="segment-representations">Segment Representations</h2>

<p>一段語音輸入進wav2vec 2.0以後，我們可以得到一串vector sequence，而作者們把所有訓練資料裡面的語音都丟進去wav2vec 2.0，將產生出來的一堆向量拿去做k-means，可以說是對每一小段語音訊號都給它一個編號。如果我們發現到說一段語音裡面相鄰兩個feature的編號相同，就可以將他們視為在同一個segment裡面，直到碰到不同編號。</p>

<p>作者們將同一個segment裡面的feature做平均來當作是這個segment的representation，值得一提的是，作者們並不是直接使用wav2vec 2.0產生出來的feature做平均，而是先對所有訓練資料經過wav2vec 2.0產生出的feature做PCA，用經過PCA轉換的feature再來做平均以後當作segment的representation。</p>

<h2 id="gan">GAN</h2>

<p><img src="./gan_framework.png" alt="GAN Framework" /></p>

<p>在得到了segment representation以後，接下來作者們將這些representation輸入進generator裡面，希望generator直接產生出phoneme distribution，這邊再搭配實際上文字的所轉換出來的phoneme 1-hot encoding，透過discriminator來讓generator產生出的phoneme distribution能越像真實的phoneme越好。另外在這邊作者們有將generator產生出來、argmax以後是相同的output再做一次平均，之後才輸入進discriminator中。</p>

<h2 id="experiment-results">Experiment Results</h2>

<p><img src="./librispeech_results.png" alt="Librispeech Results" /></p>

<p>在經過繁複的訓練流程以後，可以看到wav2vec U的表現不俗，搭配厲害的language model以後，error rate可以媲美數年前supervised learning的結果。</p>

<p><img src="./low_resource_languages_results.png" alt="Low Resource Languages Results" /></p>

<p>在low-resource上的表現甚至可以贏過supervised learning的結果。</p>

<h2 id="參考資料">參考資料</h2>

<ol>
  <li><a href="http://www.fabwrite.com/productquantization">实例理解product quantization算法</a></li>
</ol>]]></content><author><name>Your Name</name></author><category term="Paper" /><category term="Speech-Recognition" /><summary type="html"><![CDATA[簡單介紹一下Facebook AI Research前陣子出的、以非監督的方式來做語音辨識的paper。]]></summary></entry><entry><title type="html">Object Detection概論</title><link href="https://wjohn1483.github.io/#%20the%20base%20hostname%20&%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2021/07/25/object-detection/" rel="alternate" type="text/html" title="Object Detection概論" /><published>2021-07-25T00:00:00+00:00</published><updated>2021-07-25T00:00:00+00:00</updated><id>https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2021/07/25/object-detection</id><content type="html" xml:base="https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2021/07/25/object-detection/"><![CDATA[<p>簡單整理一下目前有關object detection的一些研究。</p>

<!--more-->

<h2 id="暴力做object-detection">暴力做Object Detection</h2>

<p>隨著深度學習的興起，現在已經有各式各樣的model可以幫助我們做image classification，辨識出圖片裡面的是什麼東西。如果想要直接用這些model做物件偵測，一個簡單暴力的方式是，用不同大小的方框掃過整張圖片，把每一個方框的圖片都丟進去model裡面做物件分類來達到物件偵測的效果。</p>

<h2 id="region-with-cnnr-cnn">Region with CNN（R-CNN）</h2>

<p>在前面提到我們可以用不同的方框掃過整張圖片來做物件偵測，但這樣的效率明顯不高，而且很吃計算量，這邊要介紹的<a href="https://arxiv.org/pdf/1311.2524.pdf">R-CNN</a>便是透過Selective Search來選取可能有東西的區域，再拿去model裡面分類，其大致上的流程如下：</p>

<ol>
  <li>產生大約2000個可能有東西的區域（Region Proposals）</li>
  <li>透過預先訓練好的模型像是AlexNet、Inception等抽取圖片特徵</li>
  <li>將特徵透過SVM來去分辨裡面有沒有含有特定物體</li>
</ol>

<p><img src="./rcnn_architecture.png" alt="R-CNN Architecture" /></p>

<h3 id="selective-search">Selective Search</h3>

<p><a href="https://ivi.fnwi.uva.nl/isis/publications/2013/UijlingsIJCV2013/UijlingsIJCV2013.pdf">Selective search</a>是以<a href="http://cs.brown.edu/people/pfelzens/papers/seg-ijcv.pdf">Graph Based Segmentation</a>的結果為基礎，使用階層群聚演算法來得到要輸入進model的region proposal，Graph Based Segmentation在Open CV裡面有支援，使用的方式可以參考<a href="https://blog.gtwang.org/programming/opencv-graph-based-segmentation-tutorial/">這篇部落格</a>。</p>

<p>階層群聚演算法會從Graph Based Segmentation的結果裡面，每次挑選出兩個最相近的區塊做合併，而相似度的計算包含了顏色、紋理、大小等等的因素，綜合起來來判斷兩個區塊的相似度，詳細的介紹可以參考<a href="https://blog.gtwang.org/programming/selective-search-for-object-detection/">這篇部落格</a>，具體演算法的流程可以參考下圖。</p>

<p><img src="./selective_search_algorithm.png" alt="Selective Search Algorithm" /></p>

<h3 id="bounding-box-regression">Bounding-box Regression</h3>

<p>R-CNN除了使用selective search找到有可能的region proposal以外，有另外透過bounding-box regression來調整selective search框出來的bounding box。</p>

<p>對於每一個bounding box，我們可以透過\((x, y, w, h)\)​來表示，這四個數值分別代表這個bounding box中心點座標和長寬，而bounding-box regression是想要透過一個平移加縮放的mapping function \(f()\)​來把selective search得到的bounding box \((P_x, P_y, P_w, P_y)\)透過轉換以後可以越接近label好的bounding box \((G_x, G_y, G_w, G_h)\)​越好。</p>

\[f(P_x, P_y, P_w, P_y)=(\hat{G}_x, \hat{G}_y, \hat{G}_w, \hat{G}_h)\approx(G_x, G_y, G_w, G_h)\]

<p>透過將\(f()\)​弄成regression task，之後便可以透過gradient descent來訓練\(f()\)​，更詳細的說明可以參考<a href="https://blog.csdn.net/zijin0802034/article/details/77685438">這篇文章</a>。</p>

<h2 id="fast-r-cnn">Fast R-CNN</h2>

<p>在上面的R-CNN裡面，每一個region proposal都會被丟進預先訓練好的模型來抽取圖片特徵，然而很多時候region proposal之間會有重疊的部分，其實並不需要重新再算一遍，因此在<a href="https://arxiv.org/pdf/1504.08083.pdf">Fast R-CNN</a>裡面想要直接把region proposal對應到feature map中，來避免掉不必要的計算。</p>

<p><img src="./fast_rcnn_architecture.png" alt="Fast R-CNN Architecture" /></p>

<p>Fast R-CNN跟R-CNN一樣，會先需要先選好region proposal，在把圖片經過CNN得到feature map的時候，也會將選好的region proposal也投影到跟feature map一樣的大小，直接對該區塊內的feature map做max pooling後得到固定大小的feature，最後透過fully connected layers來得到region裡面物體的類別，以及對bounding box做回歸，這個方法被稱作Region of Interest Pooling（RoI Pooling）。</p>

<p><img src="https://deepsense.ai/wp-content/uploads/2017/02/roi_pooling-1.gif" alt="RoI Pooling" /></p>

<p><em><a href="https://deepsense.ai/region-of-interest-pooling-explained/">Region of interest pooling explained</a></em></p>

<h2 id="faster-r-cnn">Faster R-CNN</h2>

<p>在Fast R-CNN裡面還是會需要跑selective search來得到region proposal，而<a href="https://arxiv.org/pdf/1506.01497.pdf">Faster R-CNN</a>的想法是想要直接在CNN的feature map裡面透過Region Proposal Network來直接得到region proposal。</p>

<p>Region Proposal Network是一個CNN，它會先用個sliding window去掃過整張圖片，在每個window的中心點去套用\(k\)​​個預先定義好、不同大小的anchor box，將anchor box框出來的feature map輸入進CNN以後，predict出含有物體的機率，以及實際bounding box的座標。</p>

<p><img src="./region_proposal_network.png" alt="Region Proposal Network" /></p>

<p>其整體的架構畫出來大概長底下這樣，中間的區塊是Faster R-CNN提出來的RPN，而右邊的是原本Fast R-CNN既有的部分。</p>

<p><img src="./faster_rcnn_graph.png" alt="Faster R-CNN Graph" /></p>

<p><em><a href="https://blog.csdn.net/jiongnima/article/details/79094159">实例分割模型Mask R-CNN详解：从R-CNN，Fast R-CNN，Faster R-CNN再到Mask R-CNN</a></em></p>

<h2 id="mask-r-cnn">Mask R-CNN</h2>

<p><a href="https://arxiv.org/pdf/1703.06870.pdf">Mask R-CNN</a>在Faster R-CNN的基礎上，修改了RoI Pooling，調整成RoI Align，並多加入Mask prediction。</p>

<p><img src="./mask_rcnn_graph.png" alt="Mask R-CNN Graph" /></p>

<p><em><a href="https://blog.csdn.net/jiongnima/article/details/79094159">实例分割模型Mask R-CNN详解：从R-CNN，Fast R-CNN，Faster R-CNN再到Mask R-CNN</a></em></p>

<h3 id="roi-align">RoI Align</h3>

<p>在原本RoI Pooling的使用情境下，會需要在把region proposal投影到feature map的維度時把座標取整數，在計算max pooling的時候也需要對區塊的範圍取整數，雖然說在feature map上面取整數去除掉的零頭看起來不大，但feature map實際上是圖片經過了很多CNN，濃縮過的feature，去除掉的零頭在原本圖片上的影響其實是比想像中大的。</p>

<p>因此在Mask R-CNN裡面使用bilinear interpolation來避免掉取整數的問題，下圖中的藍色方框是CNN輸出的feature map，而黑色方框是RoI，藉由interpolation的方式來計算出要輸入至後面的feature。</p>

<p><img src="roi_align.png" alt="RoI Align" /></p>

<h3 id="mask-prediction">Mask Prediction</h3>

<p>在Mask R-CNN做的另一件事情是引入<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf">Fully Convolutional Networks（FCN）</a>來做pixel-wise的mask，FCN主要是先使用CNN來對圖片做降維，之後再透過deconvolution CNN來把feature還原到圖片原本的大小，並predict出segmentation。</p>

<p><img src="./fcn_diagram.png" alt="FCN Diagram" /></p>

<h2 id="yolo">YOLO</h2>

<p>前面所提的R-CNN們都是region-based的方法，會需要先決定有可能的region proposal再往後進行下去，而<a href="https://pjreddie.com/media/files/papers/yolo.pdf">YOLO</a>是You Only Look Once的縮寫，一個region-free的方法，希望可以只跑一次CNN就同時產生出bounding box和物體的類別。</p>

<p><img src="yolo_model.png" alt="YOLO Model" /></p>

<p>首先，YOLO會先把圖片切成\(S \times S\)​個grid，並讓每一個grid都用neural network去預測\(B\)​個可能的bounding box、含有物件的信心程度以及物件的類別。在圖中上半部不同的黑框便是預測出來的bounding box，而不同的粗細表示信心程度的大小，而下半部是物件類別的預測結果。</p>

<p>在選出了有可能的bounding box和類別偵測的結果以後，會先用一個固定的threshold把信心程度太低的bounding box去除掉，然後使用Non-Maximum Supression（NMS）來計算出最後的輸出結果，NMS運作的方式可以參考<a href="https://chih-sheng-huang821.medium.com/%E6%A9%9F%E5%99%A8-%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-%E7%89%A9%E4%BB%B6%E5%81%B5%E6%B8%AC-non-maximum-suppression-nms-aa70c45adffa">這篇文章</a>，裡頭有詳細的圖解可以參考。</p>

<h2 id="參考資料">參考資料</h2>

<ol>
  <li><a href="https://blog.gtwang.org/programming/opencv-graph-based-segmentation-tutorial/">OpenCV 教學：實作 Graph Based Segmentation 圖形分割演算法</a></li>
  <li><a href="https://blog.gtwang.org/programming/selective-search-for-object-detection/">OpenCV 教學：實作 Selective Search 物體偵測候選區域演算法</a></li>
  <li><a href="https://blog.csdn.net/zijin0802034/article/details/77685438">边框回归(Bounding Box Regression)详解</a></li>
  <li><a href="https://ccshenyltw.medium.com/object-detection-r-cnn-fast-rcnn-faster-rcnn-mask-rcnn-retinanet-to-be-continued-71b67640445">Object Detection : R-CNN, Fast-RCNN, Faster RCNN</a></li>
  <li><a href="https://medium.com/cubo-ai/%E7%89%A9%E9%AB%94%E5%81%B5%E6%B8%AC-object-detection-740096ec4540">關於影像辨識，所有你應該知道的深度學習模型</a></li>
  <li><a href="https://blog.csdn.net/jiongnima/article/details/79094159">实例分割模型Mask R-CNN详解：从R-CNN，Fast R-CNN，Faster R-CNN再到Mask R-CNN</a></li>
  <li><a href="https://deepsense.ai/region-of-interest-pooling-explained/">Region of interest pooling explained</a></li>
  <li><a href="https://chih-sheng-huang821.medium.com/%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-%E7%89%A9%E4%BB%B6%E5%81%B5%E6%B8%AC-you-only-look-once-yolo-4fb9cf49453c">深度學習-物件偵測:You Only Look Once (YOLO)</a></li>
  <li><a href="https://chih-sheng-huang821.medium.com/%E6%A9%9F%E5%99%A8-%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-%E7%89%A9%E4%BB%B6%E5%81%B5%E6%B8%AC-non-maximum-suppression-nms-aa70c45adffa">機器/深度學習: 物件偵測 Non-Maximum Suppression (NMS)</a></li>
</ol>]]></content><author><name>Your Name</name></author><category term="Computer-Vision" /><category term="Object-Detection" /><summary type="html"><![CDATA[簡單整理一下目前有關object detection的一些研究。]]></summary></entry><entry><title type="html">在遠端的job執行完成時發送通知</title><link href="https://wjohn1483.github.io/#%20the%20base%20hostname%20&%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2021/07/25/remote-job-finish-notification/" rel="alternate" type="text/html" title="在遠端的job執行完成時發送通知" /><published>2021-07-25T00:00:00+00:00</published><updated>2021-07-25T00:00:00+00:00</updated><id>https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2021/07/25/remote-job-finish-notification</id><content type="html" xml:base="https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2021/07/25/remote-job-finish-notification/"><![CDATA[<p>這邊記錄一個方法來當server上的job跑完時，可以在MacBook上面跳出一個通知。</p>

<!--more-->

<h2 id="方法簡介">方法簡介</h2>

<p>使用的方法是透過在SSH連線至遠端server的時候，順便做port forwarding，把自己電腦的port 22接上遠端server，如此便可以讓遠端server透過SSH連線回電腦上使用command line跳出通知。</p>

<h2 id="在macbook上面允許ssh連線">在MacBook上面允許SSH連線</h2>

<p>首先，我們必須要讓MacBook可以接受SSH離線，只需要在<code class="language-plaintext highlighter-rouge">設定→共享</code>裡面打開SSH連線的設定就可以了，在<a href="https://support.apple.com/zh-tw/guide/mac-help/mchlp1066/mac">Apple官方的使用手冊</a>上有詳細的說明。</p>

<h2 id="forward-local-port">Forward Local Port</h2>

<p>在SSH連線到遠端server的時候，可以多加<code class="language-plaintext highlighter-rouge">-R</code>這個option，便可以把本機的port接到遠端server上。</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh <span class="nt">-R</span> 2000:localhost:22 &lt;username&gt;@&lt;<span class="nb">hostname</span><span class="o">&gt;</span>
</code></pre></div></div>

<p>在上面的指令當中，便是將遠端server的port 2000跟本機的port 22做連結。</p>

<h2 id="從server連回本機">從Server連回本機</h2>

<p>在連上server以後，可以先試著將底下的<code class="language-plaintext highlighter-rouge">username</code>換成本機的使用者名稱測試看看能不能連回來。</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh <span class="nt">-p</span> 2000 &lt;username&gt;@localhost
</code></pre></div></div>

<p>如果能順利連回來的話，接下來便是把public key放到本機裡面，以避免每次連線都要打密碼，詳細的流程看底下的步驟，主要是參考<a href="https://help.dreamhost.com/hc/en-us/articles/216499537-How-to-configure-passwordless-login-in-Mac-OS-X-and-Linux">這篇文章</a>。</p>

<ol>
  <li>
    <p>在server上使用底下的指令創造key</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> ssh-keygen
</code></pre></div>    </div>
  </li>
  <li>
    <p>把創建出來，帶有<code class="language-plaintext highlighter-rouge">.pub</code>副檔名的檔案裡面所有的內容複製進本機的<strong>~/.ssh/authorized_keys</strong>這份檔案中，如果這個檔案原本不存在，可以直接用文字編輯器建立</p>
  </li>
  <li>
    <p>在server上透過key來連線至本機</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> ssh <span class="nt">-i</span> &lt;path to private key&gt; <span class="nt">-p</span> &lt;port&gt; &lt;username&gt;@localhost
</code></pre></div>    </div>

    <p>預設private key的路徑會是<strong>~/.ssh/id_rsa</strong>。</p>
  </li>
</ol>

<h2 id="傳送notification">傳送Notification</h2>

<p>在能順利從server連回本機以後，就可以透過command line來傳送notification了，底下的指令是使用MacBook原生的指令來產生notification，其他argument可以參考<a href="https://code-maven.com/display-notification-from-the-mac-command-line">這篇文章</a>。</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>osascript <span class="nt">-e</span> <span class="s1">'display notification "" with title "Job Finished!" subtitle ""'</span>
</code></pre></div></div>

<p>在通知跳出來以後，可以對著通知按右鍵對通知做設定。</p>

<h2 id="總結">總結</h2>

<p>上面使用了一個簡單的方式來讓server控制本機發送通知，可以將連線、發送通知寫成腳本，並在server完成job時呼叫這個腳本來提醒你job已經跑完了。</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>Your <span class="nb">command</span><span class="o">]</span> <span class="o">||</span> send_notification.sh
</code></pre></div></div>

<h2 id="參考資料">參考資料</h2>

<ol>
  <li><a href="https://serverfault.com/questions/175798/ssh-back-to-the-local-machine-from-a-remote-ssh-session">SSH back to the local machine from a remote SSH session</a></li>
</ol>]]></content><author><name>Your Name</name></author><category term="Tool" /><summary type="html"><![CDATA[這邊記錄一個方法來當server上的job跑完時，可以在MacBook上面跳出一個通知。]]></summary></entry><entry><title type="html">深度學習於語音辨識</title><link href="https://wjohn1483.github.io/#%20the%20base%20hostname%20&%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2021/07/18/speech-recognition-with-deep-learning/" rel="alternate" type="text/html" title="深度學習於語音辨識" /><published>2021-07-18T00:00:00+00:00</published><updated>2021-07-18T00:00:00+00:00</updated><id>https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2021/07/18/speech-recognition-with-deep-learning</id><content type="html" xml:base="https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2021/07/18/speech-recognition-with-deep-learning/"><![CDATA[<p>在<a href="https://wjohn1483.github.io/2021/06/14/introduction-to-speech-recognition/">先前的文章</a>裡頭，介紹了在深度學習蓬勃發展之前，語音辨識大概是如何達成的，而這篇文章會簡單介紹一下，在加入了深度學習以後，語音辨識的技術有了什麼樣的更動。</p>

<!--more-->

<p>過去在做語音辨識的時候，會需要不同的模型互相協作，而隨著深度學習的發展，有些語音辨識的方法已經可以將所有的模型整合成一個模型，大大簡化了語音辨識的複雜度。</p>

<h2 id="listen-attend-and-spelllas">Listen, Attend and Spell（LAS）</h2>

<p>第一個要提到的模型是LAS，是一個End-to-end的語音辨識模型，輸入語音訊號，輸出就是character。</p>

<p><img src="./LAS_model_architecture.png" alt="LAS Model Architecture" /></p>

<p>上圖為LAS模型的架構，其實就是一個sequence to sequence的model搭配attention，不過比較特別的是，在encoder也就是上圖中的Listener裡頭，使用的是pyramidal bidirectional LSTM，因為輸入的語音訊號的個數通常都遠大於輸出的字數，所以透過pyramidal的方式來減少decoder attend的數量；另一個特別的地方是，在decoder的也就是上圖中的Speller裡面，仍然會將RNN的輸出拉回來放進下一個時間點的輸入。</p>

<p>LAS的model雖然可以幫助我們做語音辨識，但這個模型需要將整個句子都聽完才可以開始辨識，如果輸入進來的語音訊號很長，或是想要對串流語音做語音辨識，就得要使用其他的方式，可以參考底下的方法。</p>

<h2 id="connectionist-temporal-classificationctc">Connectionist Temporal Classification（CTC）</h2>

<p>CTC是一種基於RNN loss function的方法，可以把比較長的輸入\(X\)轉換成比較短的輸出\(Z\)，跟語音辨識所需要的條件相同，而CTC的特點在於輸出的時候會多一個<code class="language-plaintext highlighter-rouge">blank</code>的符號，每一個時間點RNN的輸出除了可以是vocabulary裡面的字以外，還可以是<code class="language-plaintext highlighter-rouge">blank</code>，寫作<code class="language-plaintext highlighter-rouge">-</code>。</p>

<p><img src="./CTC_model_architecture.png" alt="CTC Model Architecture" /></p>

<p>假如說我們拿一段語音訊號\(X=[x_1, x_2, ..., x_T]\)輸入到LSTM裡面，每個時間點LSTM都會輸出一個長度為\(L+1\)的向量\(y_t=[y_t^{-}, y_t^1, ..., y_t^L]\)，其中\(L\)表示vocabulary的大小，雖然我們知道這整段語音訊號\(X\)所對應到的文字是\(Z=[z_1, z_2, ..., z_U]\)，但因為LSTM整個輸出的長度跟輸入長度相同，都是\(T\)，而文字的長度是\(U\)，兩者對不起來，沒辦法直接一對一對應做gradient descent，所以CTC在這裡使用了<code class="language-plaintext highlighter-rouge">blank</code>並將\(Z\)裡面的文字重複來把\(Z\)的長度擴充到跟\(T\)一樣，並搭配一個mapping function \(B()\)來把所有<code class="language-plaintext highlighter-rouge">blank</code>和相鄰的重複輸出去除掉，保留不相鄰的輸出。</p>

<p>舉例來說，假設今天放進LSTM的\(X\)長度為4，而對應的\(Z\)為\([讚, 啦]\)，我們可以透過加入<code class="language-plaintext highlighter-rouge">-</code>或重複\(Z\)裡面的字來讓長度變為4，之後再讓\(B()\)幫我們把修改過的輸出\(Z^*\)的輸出對回\(Z\)，亦即</p>

\[B([-, 讚, 啦, 啦])=[讚, 啦] \\
B([讚, 讚, 啦, 啦])=[讚, 啦] \\
B([-, 讚, 啦, -])=[讚, 啦]\]

<p>其中每一個\(B()\)的輸入\([-, 讚, 啦, 啦]\)、\([讚, 讚, 啦, 啦]\)等都可以是LSTM用來學習的目標，而我們會透過<a href="https://en.wikipedia.org/wiki/Forward%E2%80%93backward_algorithm">Forward-backward Algorithm</a>來將所有可能的組合都囊括進來。</p>

<p>我們將LSTM每一個時間點的輸出\(y_t\)攤開來，並取出所有在\(Z\)裡面文字的機率，再在每個label之間都插入<code class="language-plaintext highlighter-rouge">blank</code>的符號，如上圖中的上半部。在這邊，我們限定每個時間點的轉移只能往右邊或右上角走，不能往右下或左邊走，如此便可以透過Forward Algorithm將最後一個時間點的機率總和來當作觀察到輸出\(Z\)的機率\(P(Z\vert X)\)，其中每一個可能的\(Z^*\)的機率可以被表示為</p>

\[P(Z^*\vert X)=\coprod\limits_{t=1}\limits^{T}y_t^{Z^*_t}\]

<p>而\(P(Z\vert X)\)可以被表示為</p>

\[P(Z\vert X)=\sum\limits_{Z^* \in B^{-1}(Z)}P(Z^*\vert X)\]

<p>最後就可以透過底下的loss function做訓練了</p>

\[\mathcal{L}_{CTC}=-\sum\limits_{\forall(X,Z^*)\in\theta}\ln P(Z^*\vert X)\]

<p>訓練完CTC以後，就可以直接將聲音訊號餵進去，選擇每一個時間點最大的文字搭配\(B()\)當作是語音辨識的輸出，也可以使用beam search來增進performance。</p>

<h2 id="rnn-transducerrnn-t">RNN Transducer（RNN-T）</h2>

<p>在上述CTC的部分裡面，我們透過在label裡面加入<code class="language-plaintext highlighter-rouge">blank</code>來讓串流語音辨識成為可能，然而在CTC裡面，每一個時間點的輸出跟下一個時間點的輸出是互相獨立的，這會使得後面時間的要輸出的時候因為不確定前面輸出過了沒造成結巴的現象，而RNN-T所做的改動便是將上一個時間點的輸出拉回來當作是下一個時間點的輸入，並改變predict文字的方式。</p>

<p>在前面CTC的部分裡，每一個時間點LSTM會吃一個向量進來，輸出一個文字，但在RNN-T裡面，每一個時間點LSTM會重複吃一同個向量，直到predict出<code class="language-plaintext highlighter-rouge">NULL</code>以後才會吃下一個輸入。</p>

<pre><code class="language-mermaid">graph BT;
    LSTM1[LSTM]
    LSTM2[LSTM]
    LSTM3[LSTM]
    LSTM4[LSTM]
    LSTM5[LSTM]
    LSTM6[LSTM]
    i1[i1]
    i2[i2]
    i3[i3]
    o1[c]
    o2[a]
    o3[NULL]
    o4[t]
    o5[NULL]
    o6[NULL]
    i1--&gt;LSTM1
    LSTM1--&gt;o1
    i1--&gt;LSTM2
    LSTM2--&gt;o2
    i1--&gt;LSTM3
    LSTM3--&gt;o3
    i2--&gt;LSTM4
    LSTM4--&gt;o4
    i2--&gt;LSTM5
    LSTM5--&gt;o5
    i3--&gt;LSTM6
    LSTM6--&gt;o6
</code></pre>

<p>所以當輸入的語音embedding長度為\(T\)時，我們就會在對應的答案裡面加入\(T\)個<code class="language-plaintext highlighter-rouge">NULL</code>，至於要如何將\(T\)個<code class="language-plaintext highlighter-rouge">NULL</code>放進去就跟CTC的方式相同。</p>

<p>RNN-T除了predict方式的不同，還有在輸出的最後放一個language model，並把language model的輸出與下一個時間點的輸入一用餵進LSTM當中，可以看<a href="https://youtu.be/CGuLuBaLIeI?t=1870">影片中的架構</a>來獲得比較清楚的了解。</p>

<p>總的來說，RNN-T的架構如下圖，由三個部分所構成成：</p>

<ul>
  <li>Encoder：可以想成是acoustic model，也就是上面架構圖中的LSTM，計算語音訊號的特徵。</li>
  <li>Prediction Network：可以想成是language model，在<a href="https://youtu.be/CGuLuBaLIeI?t=1870">影片中的架構</a>裡面最上層的部分。</li>
  <li>Joint Network：綜合Encoder和Prediction Network的輸出決定最後的文字是什麼，在<a href="https://youtu.be/CGuLuBaLIeI?t=1870">影片中的架構</a>中是黃色的框框。</li>
</ul>

<p><img src="https://www.researchgate.net/publication/335044103/figure/fig3/AS:789632253952000@1565274408664/Recurrent-neural-network-RNN-transducer-structure-38.png" alt="RNN-T Overview" /></p>

<h2 id="neural-transducer">Neural Transducer</h2>

<p>Neural Transducer和RNN-T的作法類似，與RNN-T不同的地方在於，輸入從單一個acoustic feature變成多個acoustic feature，並在其中加入attention的機制。</p>

<p><img src="./neural_transducer_architecture.png" alt="Neural Transducer Architecture" /></p>

<p>在neural transducer裡，輸入進來的acoustic feature們會用固定大小\(W\)的window切開，分成多個block，每次模型會根據當前輸入的block中的acoustic feature來做predict，直到model predict出<code class="language-plaintext highlighter-rouge">NULL</code>以後才會輸入下一個block。</p>

<h2 id="monotonic-chunkwise-attentionmocha">Monotonic Chunkwise Attention（MoChA）</h2>

<p>MoChA又對neural transducer做了一些變形，在neural transducer裡使用固定大小的\(W\)來對acoustic feature分塊，而MoChA想讓model來自己決定block要怎麼切。</p>

<p><img src="./mocha_attention.png" alt="MoChA Attention" /></p>

<p>在上圖(a)是一般的soft attention，在輸出\(y\)的時候會對所有的input算attention。</p>

<p>圖(b)是monotonic attention，由model來決定要attend多少input，\(\otimes\)是有被選中的input，而\(\bullet\)是決定要停止的地方，下一個時間點會從上一個時間點停止的地方開始往後預測該要在哪個input停下來。</p>

<p>圖(c)是MoChA，與圖(b)的作法類似，但在這邊會設定一個window size \(W\)，在停下來的地方往前框\(W\)個feature來當作模型的輸入，在圖中的例子是設定\(W=3\)。</p>

<h2 id="總結">總結</h2>

<p>在這篇文章裡面介紹了一些End-to-end的方式來訓練語音辨識的模型，它們的辨識錯誤率跟先前的HMM-DNN hybrid模型相近，如果資料量夠大的話，end-to-end的模型還可以把錯誤率再調降，可以根據擁有的資料量來決定要使用哪一種方式來製作你的語音辨識模型。</p>

<h2 id="參考資料">參考資料</h2>

<ol>
  <li><a href="https://www.youtube.com/watch?v=CGuLuBaLIeI">[DLHLP 2020] Speech Recognition (3/7) - CTC, RNN-T and more</a></li>
  <li><a href="https://arxiv.org/pdf/1508.01211.pdf">Listen, Attend and Spell</a></li>
  <li><a href="https://www.researchgate.net/figure/Recurrent-neural-network-RNN-transducer-structure-38_fig3_335044103">Figure 4 - available via license: Creative Commons Attribution 4.0 International</a></li>
  <li><a href="https://arxiv.org/pdf/1511.04868.pdf">A Neural Transducer</a></li>
  <li><a href="https://arxiv.org/pdf/1712.05382.pdf">Monotonic Chunkwise Attention</a></li>
</ol>]]></content><author><name>Your Name</name></author><category term="Speech-Recognition" /><summary type="html"><![CDATA[在先前的文章裡頭，介紹了在深度學習蓬勃發展之前，語音辨識大概是如何達成的，而這篇文章會簡單介紹一下，在加入了深度學習以後，語音辨識的技術有了什麼樣的更動。]]></summary></entry><entry><title type="html">語音辨識概觀</title><link href="https://wjohn1483.github.io/#%20the%20base%20hostname%20&%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2021/06/14/introduction-to-speech-recognition/" rel="alternate" type="text/html" title="語音辨識概觀" /><published>2021-06-14T00:00:00+00:00</published><updated>2021-06-14T00:00:00+00:00</updated><id>https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2021/06/14/introduction-to-speech-recognition</id><content type="html" xml:base="https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2021/06/14/introduction-to-speech-recognition/"><![CDATA[<p>在我們的生活當中，語音辨識已經越來越普及了，不論是語音輸入、語音助手都有使用到語音辨識的技術，底下簡單介紹一下在大量使用neural network之前語音辨識所使用到的一些技術。</p>

<!--more-->

<h2 id="基礎介紹">基礎介紹</h2>

<pre><code class="language-mermaid">graph LR;
	waveform("Waveform (X)")
	word("Word (Y)")
	waveform--ASR System--&gt;word
</code></pre>

<p>語音辨識（Automatic Speech Recognition, ASR）所做的事情如上圖所示，將輸入的訊號轉換成文字，亦即是去尋找</p>

\[\arg\max\limits_Y P(Y\vert X)\]

<h3 id="輸入">輸入</h3>

<p>關於ASR輸入的部分，在大多數的情況下，我們並不會直接將waveform當作輸入，而是會先對這個waveform做一些處理，最常見的做法是將waveform轉成MFCC sequence後再餵入ASR系統裡面。</p>

<p><strong>Mel-frequency Cepstrum Coefficients (MFCC)</strong></p>

<p>在一般的設定當中，語音訊號的採樣頻率是16k，也就是1秒會有16k個數值，而<a href="https://en.wikipedia.org/wiki/Mel-frequency_cepstrum">MFCC</a>會以25ms為一個window、每10ms為間隔去將25ms的音訊轉成39維的向量。概括來說MFCC做的事情是先將waveform過傅立葉轉換，再通過一群filter bank，最後從頻域轉回來，而這些filter bank是過去人們研究耳朵的構造所訂定出來的，專門為了辨識音訊所設計。</p>

<p>MFCC具體上做的事情還蠻複雜的，好在現在可以直接使用<a href="https://github.com/kaldi-asr/kaldi">Kaldi</a>幫我們將音訊轉成MFCC，只要將音訊餵進去，它就可以幫忙將音訊轉成MFCC sequence，也就是一堆39維的向量組成的序列，接著就可以開始製作ASR系統了。</p>

<p>值得一提的是，近期有蠻多研究會直接使用過完filter bank以後得到的輸出當作ASR系統的輸入，可以看需求來選擇不同的feature，在Kaldi裡面都有支援。</p>

<h3 id="輸出">輸出</h3>

<p>在ASR系統輸出的部分，我們最終想要輸出的是文字，但在語音辨識裡面，通常ASR系統輸出的並不會直接是文字，而是比文字再更小的單位，因為一個字通常是由很多音來組成的，像是注音符號就是一個比文字再更小的單位，底下介紹一下常用的輸出有什麼。</p>

<p><strong>Phoneme</strong></p>

<p>Phoneme是發音的最小單位，可以想像成是類似英文的音標，而這種輸出會需要有該語言的專業人士告訴我們每一個字所對應到的phoneme是什麼，好讓ASR系統在產生出phoneme以後，能藉由查表的方式轉換成文字。</p>

<p><strong>Graphme</strong></p>

<p>Graphme是書寫的最小單位，在英文的話會是英文字母，在中文的話會是每一個中文字，雖說中文字有上萬個，但實際上常用的中文字大概在3000-5000的量級。</p>

<p><strong>Morpheme</strong></p>

<p>Morpheme是比graphme再大一點的單位，可以說是英文的字根字首。</p>

<p><code class="language-plaintext highlighter-rouge">unbelievable</code>→<code class="language-plaintext highlighter-rouge">un</code>、<code class="language-plaintext highlighter-rouge">believe</code>、<code class="language-plaintext highlighter-rouge">able</code></p>

<h2 id="hybrid-asr">Hybrid ASR</h2>

<p>接下來稍微講一下ASR系統該怎麼做，不過在做語音辨識之前，我們會要先去蒐集語音及其相對應的文字\(\mathcal{D}_l=\{(X_i, Y_i)\}^{N}_{i=1}\)，其中\(X=[x_1, ..., x_T]\in (\mathbb{R}^m)^*\)，\(x_t\)是一個\(m\)維的向量，可以想成是之前提到的MFCC，而\(Y=[y_1, ..., y_L] \in B^*\)，\(B\)代表的是文字的vocabulary，我們想要製作的ASR系統其實就是\(p_\theta(Y\vert X)\)，將音訊輸入進去以後，告訴我們文字出現的機率。</p>

<p>這個章節想要提的hybrid ASR主要由三個部分組成：Language Model（LM）、Pronunciation Model（PM）、Acoustic Model（AM）。</p>

<h3 id="language-model">Language Model</h3>

<p>Language model做的事情是判斷這個文字序列\(Y\)出現的機率有多少，有點像是鍵盤輸入法的校正，隨著輸入的文字越來越長，去找最適當的文字該要是什麼，寫成數學式如下。</p>

\[p_{LM}(Y)=\prod^L_{i=1}p(y_i\vert y_1, ..., y_{i-1})\]

<h3 id="pronunciation-model">Pronunciation Model</h3>

<p>Pronunciation model其實就是前面所提到的，文字跟phoneme的對應表，又稱為lexicon。每一個\(Y\)，都可以根據這個表對應成phoneme sequence。</p>

\[P=[p_1, ..., p_M]=f_{PM}(Y)\in O^*\]

<p>其中\(f_{PM}(Y)\)代表mapping function，\(O\)表示phoneme inventory，也就是所有的phoneme有哪些。</p>

<h3 id="acoustic-model">Acoustic Model</h3>

<p>Acoustic model做的事情是去決定phoneme sequence產生出\(X\)的機率，亦即\(p_{AM}(X\vert P)\)，而phoneme sequence當中的每一個phoneme \(p\)都有可能產生出不定長度的feature \(x\)，我們並不知道\(X=[x_1, ..., x_T]\)當中的哪些\(x_t\)是由哪個\(p_t\)產生的，為此，我們對於每一個phoneme都用一個<a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#Applications">Hidden Markov Model（HMM）</a>來model。</p>

<p>可以想成是說，我們創造了另一個比phoneme更小的單位，HMM的<code class="language-plaintext highlighter-rouge">state</code>，來代表一個phoneme，可能一個phoneme 是由3個state組成\(p_1=[a_1, a_2, a_3]\)，當\(X=[a_1, a_1, a_2, a_2, a_2, a_3]\)，我們就有機會可以說\(X\)是由phoneme \(p_1\)產生出來的，因此原先的phoneme sequence \(P\)，可以以HMM state來改寫成\(A=[a_1, ..., a_T]\)，這邊\(A\)的長度與\(X\)的長度一致，所以\(A\)又被稱之為alignment。</p>

\[\begin{aligned} 
p_{AM}(X,A\vert P) &amp; =p_{emit}(X\vert A)p_{tran}(A\vert P) \\
&amp; =\prod\limits_t p_{emit}(x_t\vert a_t)p_{tran}(a_t\vert a_{t-1}; P)
\end{aligned}\]

<p>有了alignment以後，我們可以根據這些state來算出joint probability，上面的\(p_{tran}\)表示的是HMM當中的transition probability，而\(p_{emit}\)表示emission probability，亦即這個state產生出這個feature的機率，通常使用<a href="https://scikit-learn.org/stable/modules/mixture.html">Gaussian Mixture Model（GMM）</a>來model。</p>

<p>而實際上我們想要知道的\(p_{AM}(X\vert P)\)就是將所有有可能的\(A\)都加起來</p>

\[\begin{aligned}
p_{AM}(X\vert P) &amp; =\sum_A p_{emit}(X\vert A)p_{tran}(A\vert P) \\
&amp; \approx \max_A p_{emit}(X\vert A)p_{tran}(A\vert P)
\end{aligned}\]

<p>雖然在算式裡面有很多的\(\sum\)和\(\prod\)還有窮舉，其實在做計算的時候會使用<a href="https://en.wikipedia.org/wiki/Forward%E2%80%93backward_algorithm">forward-backward algorithm</a>和<a href="https://en.wikipedia.org/wiki/Viterbi_algorithm">Viterbi algorithm</a>這些類似dynamic programming的方式來有效率的計算。</p>

<h3 id="組合hybrid-asr">組合Hybrid ASR</h3>

<p>前面介紹了hybrid ASR當中重要的三個部分：LM、PM、AM，而它們三個組合起來的方式如下</p>

\[\begin{aligned}
p_\theta(X,Y) &amp; = p_{AM}(X\vert f_{PM}(Y))p_{LM}(Y) \\
&amp; = \sum_Ap_{emit}(X\vert A)p_{tran}(A\vert f_{PM}(Y))p_{LM}(Y) \\
&amp; \approx \max_Ap_{emit}(X\vert A) p_{tran}(A\vert f_{PM}(Y))p_{LM}(Y)
\end{aligned}\]

<p>雖說它們存在於同一個數學式當中，但實際上LM和AM通常是會被分開訓練的，它們有各自的objective function。</p>

\[p^*_{LM}=\arg\max\limits_{p_{LM}}\sum\limits^{N}\limits_{i=1}\log p_{LM}(Y_i)\]

\[p^*_{AM}=\arg\max\limits_{p_{AM}}\sum\limits^N\limits_{i=1}\log p_{AM}(X_i\vert f_{PM}(Y_i))\]

<p>在知道怎麼計算joint probability \(p_\theta(X,Y)\)以後，我們想要求得的\(\arg \max\limits_Y p_\theta(Y\vert X)\)就是去尋找最大的joint probability \(p_\theta(X,Y)\)。</p>

\[\begin{aligned}
\arg\max\limits_Yp_\theta(Y\vert X) &amp; = \arg\max\limits_Y\frac{p_\theta(X,Y)}{p(X)} \\
&amp; \approx \arg\max\limits_Yp_\theta(X,Y)
\end{aligned}\]

<p>其中\(p(X)\)是語音\(X\)出現的機率，假設每一段語音出現的機率都均等的話，我們可以忽略不看。</p>

<p>實際上在操作的時候，我們會將HMM emission probability、transition probability、lexicon和language model共同組成<a href="https://en.wikipedia.org/wiki/Finite-state_transducer">Weighted Finite State Transducers（WFST）</a>，並從中去尋找最佳路徑，與此同時還會引入一個\(\alpha\)來去調配acoustic model和language model的平衡。</p>

\[p_{\theta,\alpha}(X,Y)\propto p_{AM}(X\vert f_{PM}(Y))p_{LM}(Y)^\alpha\]

<p>而這個\(\alpha\)是在development set \(\{(\tilde X_i,\tilde Y_i)\}^{\tilde N}_{i=1}\)最小化edit distance得到的。</p>

\[\alpha=\arg\min\limits_\alpha\sum\limits_{i=1}\limits^{\tilde N}ED\left ( \tilde Y_i,\arg\max\limits_Y \left (\log p_{AM}(\tilde X\vert f_{PM}(Y))+ \alpha*\log p_{LM}(Y)\right ) \right )\]

<p>所以最終語音辨識出來的文字會是</p>

\[Y^*=\arg\max\limits_Yp_{\theta,\alpha}(X,Y)\]

<h3 id="在hybrid-asr裡面使用dnn">在Hybrid ASR裡面使用DNN</h3>

<p>隨著neural network的興起，語音辨識也逐漸引入deep learning來幫助辨識，在這個hybrid ASR系統裡面，我們也可以使用DNN來幫助我們提高辨識率。</p>

<p>在acoustic model裡面，我們想要算的是\(p_{AM}(X\vert P)\)，在前面的推導當中，我們得到</p>

\[p_{AM}(X\vert P)\approx \max_A p_{emit}(X\vert A)p_{tran}(A\vert P)\]

<p>其中再去細鑽下去的話</p>

\[\begin{aligned}
p_{AM}(X\vert P) &amp; \approx \max_A p_{emit}(X\vert A)p_{tran}(A\vert P) \\
&amp; = \max\limits_A\prod\limits^T\limits_{t=1}p_{emit}(x_t\vert a_t)p_{tran}(a_t\vert a_{t-1}; P) \\
&amp; = \max\limits_A\prod\limits^T\limits_{t=1}\frac{p(a_t\vert x_t)p(x_t)}{p(a_t)}p_{tran}(a_t\vert a_{t-1}; P)
\end{aligned}\]

<p>其中\(p_{tran}(a_t\vert a_{t-1}; P)\)一樣是由HMM來，\(p(a_t)\)可以從HMM裡面統計出來\(p(a_t)=\frac{a_t出現的次數}{所有state出現的次數}\)，而\(p(a_t\vert x_t)\)就可以用DNN來做訓練。</p>

<h2 id="參考資料">參考資料</h2>

<ol>
  <li><a href="https://www.youtube.com/watch?v=AIKu43goh-8">[DLHLP 2020] Speech Recognition (1/7) - Overview</a></li>
  <li><a href="https://arxiv.org/abs/2105.11084">Unsupervised Speech Recognition</a></li>
</ol>]]></content><author><name>Your Name</name></author><category term="Speech-Recognition" /><summary type="html"><![CDATA[在我們的生活當中，語音辨識已經越來越普及了，不論是語音輸入、語音助手都有使用到語音辨識的技術，底下簡單介紹一下在大量使用neural network之前語音辨識所使用到的一些技術。]]></summary></entry><entry><title type="html">在Google Sheet裡面呈現html</title><link href="https://wjohn1483.github.io/#%20the%20base%20hostname%20&%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2021/06/05/display-html-in-google-sheet/" rel="alternate" type="text/html" title="在Google Sheet裡面呈現html" /><published>2021-06-05T00:00:00+00:00</published><updated>2021-06-05T00:00:00+00:00</updated><id>https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2021/06/05/display-html-in-google-sheet</id><content type="html" xml:base="https://wjohn1483.github.io/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site%20e.g.%20https://www.someone.com/2021/06/05/display-html-in-google-sheet/"><![CDATA[<p>前陣子有人推薦了一個好用的資料呈現套件<a href="https://plotly.com/">plotly</a>，這個套件可以將結果輸出成html讓使用者與之互動，這邊紀錄一下如何將html呈現在Google Sheet裡面，讓我們不用再找一個地方存放這個html。</p>

<!--more-->

<p>具體實現的方法是建立一個Google Apps Script，並綁定Google Sheet上面的物件，讓使用者跟這個物件互動的時候跳出一個Dialog擺html來跟使用者互動，底下會詳述各個步驟該如何設定。</p>

<h2 id="google-apps-script">Google Apps Script</h2>

<p>首先先打開Google Sheet，並在工具的地方按下Script editor。</p>

<p><img src="./script_editor.png" alt="Script Editor" /></p>

<p>按下去以後會跳出新的分頁，在這裡可以寫一些程式碼來去與Google的服務做互動。</p>

<p><img src="./new_app_script.png" alt="New App Script" /></p>

<p>在這邊我們將底下的程式碼貼到<code class="language-plaintext highlighter-rouge">Code.gs</code>的檔案裡面，記得將<code class="language-plaintext highlighter-rouge">&lt;YOUR_HTML_FILENAME&gt;</code>替換成你html的檔名、想一個要顯示在Dialog的名稱替換掉<code class="language-plaintext highlighter-rouge">&lt;TITLE_OF_DIALOG&gt;</code>。目前設定跳出來的Dialog的大小是1000*1000，可以根據你想要顯示的大小做調整。</p>

<div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">function</span> <span class="nx">openDialog</span><span class="p">()</span> <span class="p">{</span>
  <span class="kd">var</span> <span class="nx">html</span> <span class="o">=</span> <span class="nx">HtmlService</span><span class="p">.</span><span class="nx">createHtmlOutputFromFile</span><span class="p">(</span><span class="dl">'</span><span class="s1">&lt;YOUR_HTML_FILENAME&gt;</span><span class="dl">'</span><span class="p">)</span>
    <span class="p">.</span><span class="nx">setSandboxMode</span><span class="p">(</span><span class="nx">HtmlService</span><span class="p">.</span><span class="nx">SandboxMode</span><span class="p">.</span><span class="nx">NATIVE</span><span class="p">)</span>
    <span class="p">.</span><span class="nx">setHeight</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
    <span class="p">.</span><span class="nx">setWidth</span><span class="p">(</span><span class="mi">1000</span><span class="p">);</span>
  <span class="nx">SpreadsheetApp</span><span class="p">.</span><span class="nx">getUi</span><span class="p">()</span> 
    <span class="p">.</span><span class="nx">showModalDialog</span><span class="p">(</span><span class="nx">html</span><span class="p">,</span> <span class="dl">'</span><span class="s1">&lt;TITLE_OF_DIALOG&gt;</span><span class="dl">'</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div>

<p>最後在檔案旁邊的<code class="language-plaintext highlighter-rouge">+</code>號點選新增html，並將你想要呈現的html貼在裡面並按下旁邊的<code class="language-plaintext highlighter-rouge">Save project</code>就完成這個部分了。</p>

<p><img src="./new_html.png" alt="New html" /></p>

<p><img src="./save_project.png" alt="Save Project" /></p>

<p>記得新增的html名稱要跟程式碼裡面寫的一致。</p>

<h2 id="綁定google-sheet物件">綁定Google Sheet物件</h2>

<p>首先，我們要先創建一個物件讓使用者跟它做互動，這邊我們自己畫一個像按鈕的圖樣。</p>

<p><img src="./new_drawing.png" alt="New Drawing" /></p>

<p><img src="./drawing_example.png" alt="Draw Example" /></p>

<p>建立好物件後，點選右上角的三個點，選擇<code class="language-plaintext highlighter-rouge">Assign script</code>。</p>

<p><img src="./assign_script_dropdown.png" alt="Assign Script" /></p>

<p>在跳出的視窗填上剛剛程式碼中的function名稱<code class="language-plaintext highlighter-rouge">openDialog</code>後按下OK。</p>

<p><img src="./assign_script_popup.png" alt="Assign Script" /></p>

<p>到這邊就全部設定完成了！之後使用者點選物件就會跳出你製作的html。</p>

<p><img src="./result.gif" alt="Result" /></p>

<p>值得一提的是，在使用者第一次點選的時候，應該會跳出像底下的權限要求，照著提示准許就行。</p>

<p><img src="./authorization.png" alt="Authorization" /></p>

<p>如果之後想要移動物件位置的話，對物件按滑鼠右鍵即可。</p>

<h2 id="參考資料">參考資料</h2>

<ol>
  <li><a href="https://www.quora.com/How-do-I-add-HTML-code-in-a-Google-Spreadsheet-Cell">How do I add HTML code in a Google Spredsheet Cell</a></li>
</ol>]]></content><author><name>Your Name</name></author><category term="Tool" /><summary type="html"><![CDATA[前陣子有人推薦了一個好用的資料呈現套件plotly，這個套件可以將結果輸出成html讓使用者與之互動，這邊紀錄一下如何將html呈現在Google Sheet裡面，讓我們不用再找一個地方存放這個html。]]></summary></entry></feed>